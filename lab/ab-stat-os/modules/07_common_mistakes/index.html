<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Модуль 7. Типичные ошибки — Статистика A/B-тестирования</title>
    <link rel="stylesheet" href="../../assets/style.css">
</head>
<body>
    <div class="container">
        <div class="breadcrumbs">
            <a href="../../../../courses/index.html">Все курсы</a> /
            <a href="../../index.html">Статистика A/B-тестирования</a> / Модуль 7
        </div>

        <div class="nav-links">
            <a href="./index.html" class="active">Модуль</a>
            <a href="./practice.html">Практика</a>
            <a href="./simulators.html">Симуляторы</a>
        </div>

        <h1>Модуль 7. Типичные ошибки аналитиков</h1>

        <!-- Section 1 -->
        <section class="section">
            <h2>1. Ошибка единицы анализа</h2>

            <p>
                Одна из самых частых ловушек в продуктовой аналитике — считать метрику на одном уровне,
                а рандомизацию проводить на другом. Особенно часто это проявляется в виде <strong>fan-out</strong>:
                когда один пользователь генерирует много событий (сессий, заказов, показов) и получает
                непропорционально большой вес в тесте.
            </p>

            <h3>Как выглядит ошибка</h3>
            <p>
                Допустим, вы запустили A/B-тест на уровне пользователей (рандомизация по <code>user_id</code>),
                но используете метрику «конверсия сессии в покупку»: доля сессий, в которых была покупка.
                Пользователь, который заходит 10 раз в день, даёт в выборку в 10 раз больше сессий,
                чем пользователь с одной сессией. При этом treatment назначен обоим одинаково — как одному объекту.
            </p>
            <p>
                Аналитик считает t-test по сессиям, получает N = 5&nbsp;000&nbsp;000 сессий и p = 0.02.
                Вывод: «эффект значим, можно выкатывать». Но на уровне пользователей всё может быть совсем иначе.
            </p>

            <h3>Почему это проблема</h3>
            <ul>
                <li>Нарушается предпосылка независимых одинаково распределённых наблюдений: сессии одного пользователя
                    сильно скоррелированы.</li>
                <li>Происходит <strong>искусственное раздувание выборки</strong>: кажется, что у нас миллионы наблюдений,
                    хотя реальное число независимых единиц — количество пользователей.</li>
                <li>В результате <strong>стандартные ошибки занижаются</strong>, доверительные интервалы сжимаются,
                    а доля ложноположительных результатов резко растёт.</li>
            </ul>

            <div class="formula">
                \(\text{SE}_{\text{event-level}} &lt; \text{SE}_{\text{user-level}}\) при наличии fan-out
            </div>

            <h3>Мини-пример</h3>
            <p>
                Представьте двух пользователей:
            </p>
            <ul>
                <li>Пользователь A: 1 сессия, 1 покупка.</li>
                <li>Пользователь B: 50 сессий, 1 покупка.</li>
            </ul>
            <p>
                На уровне пользователей конверсия в покупку: A = 100%, B = 100% → средняя конверсия = 100%.
            </p>
            <p>
                На уровне сессий: всего 51 сессия и 2 покупки → конверсия сессии ≈ 3.9%.
                Пользователь с 50 сессиями получил 50 голосов вместо одного.
            </p>

            <h3>Что делать правильно</h3>
            <div class="callout">
                <p><strong>Правило:</strong> <em>единица анализа должна совпадать с единицей рандомизации</em>.</p>
            </div>
            <ul>
                <li>Если рандомизируете по пользователям — агрегируйте данные до <strong>user-level</strong>
                    (одна строка на пользователя) и считайте метрики на этом уровне.</li>
                <li>Если рандомизируете по кластерам (города, магазины, курьерские зоны) — агрегируйте
                    до уровня кластера или используйте cluster-robust стандартные ошибки.</li>
                <li>Если по бизнес-причинам вы хотите метрику на другом уровне (например, по сессиям) —
                    всё равно обеспечьте, чтобы один пользователь не получал сверхвес (обрезка fan-out, взвешивание).</li>
            </ul>

            <div class="callout callout-warn">
                <p><strong>Опасный симптом:</strong> если N в отчёте измеряется миллионами событий, а эксперимент
                    запускался «на пользователях», почти наверняка у вас ошибка единицы анализа.</p>
            </div>
        </section>

        <!-- Section 2 -->
        <section class="section">
            <h2>2. Ratio-метрики: «ratio of means» vs «mean of ratios»</h2>

            <p>
                Многие ключевые продуктовые и бизнес-метрики — это <strong>отношения</strong> двух величин:
                AOV = GMV / Orders, RPM = Revenue / Impressions, ARPU = Revenue / Users.
                Для них есть два принципиально разных способа оценки: «ratio of means» и «mean of ratios».
            </p>

            <h3>Как выглядит ошибка</h3>
            <p>
                Аналитик берёт данные на уровне пользователя и считает:
            </p>
            <div class="formula">
                AOV<sub>i</sub> = GMV<sub>i</sub> / Orders<sub>i</sub>
            </div>
            <p>
                Затем берёт простой средний:
            </p>
            <div class="formula">
                \(\text{mean of ratios} = \frac{1}{n}\sum_{i=1}^{n} AOV_i\)
            </div>
            <p>
                В этом случае пользователь с 1 заказом и пользователь со 100 заказами получают
                <strong>одинаковый вес</strong> в оценке AOV.
            </p>

            <h3>Мини-пример</h3>
            <p>Два пользователя:</p>
            <ul>
                <li>A: 1 заказ на 50&nbsp;₽ → AOV<sub>A</sub> = 50.</li>
                <li>B: 100 заказов, суммарно 50&nbsp;000&nbsp;₽ → AOV<sub>B</sub> = 500.</li>
            </ul>
            <p>
                Mean of ratios: (50 + 500) / 2 = 275&nbsp;₽.
            </p>
            <p>
                Истинный AOV как <strong>ratio of means</strong>:
            </p>
            <div class="formula">
                \(\text{ratio of means} = \frac{GMV_A + GMV_B}{Orders_A + Orders_B}
                = \frac{50 + 50\,000}{1 + 100} \approx 495\,₽\)
            </div>
            <p>
                Разница почти в два раза, потому что heavy user B «размыт» при одинаковом весе с A.
            </p>

            <h3>Почему это проблема</h3>
            <ul>
                <li><strong>Смещённая оценка.</strong> Mean of ratios отвечает на вопрос
                    «средний чек среднего пользователя», а ratio of means — «средний чек заказа».
                    В большинстве бизнес-кейсов вас интересует именно второй вариант.</li>
                <li><strong>Риск парадокса Симпсона.</strong> В разных сегментах (частота, категории)
                    mean of ratios и ratio of means могут вести себя в разные стороны.</li>
                <li><strong>Неправильный выбор теста.</strong> Наивный t-test по пользовательским AOV<sub>i</sub>
                    игнорирует сложную дисперсионную структуру ratio-метрики.</li>
            </ul>

            <h3>Что делать правильно</h3>
            <ul>
                <li>Для ratio-метрик используйте <strong>ratio of means</strong>:
                    агрегируйте числитель и знаменатель по группе и берите их отношение.</li>
                <li>Для оценки значимости применяйте:
                    <ul>
                        <li><strong>delta-method / линейризацию</strong> (как в модуле про variance reduction);</li>
                        <li>перестановочные тесты или <strong>bootstrap по пользователям</strong>, который учитывает зависимость GMV и Orders;</li>
                        <li>специализированные калькуляторы (например, симуляторы и <code>metric_classifier</code>), которые
                            подскажут тип метрики и допустимые тесты.</li>
                    </ul>
                </li>
            </ul>

            <div class="callout">
                <p><strong>Связь с симуляторами:</strong> пропустите вашу метрику через
                    <strong>«Классификатор метрик»</strong> и <strong>«Карту выбора статкритерия»</strong>,
                    чтобы проверить, не применяете ли вы t-test к сложной ratio-метрике.</p>
            </div>
        </section>

        <!-- Section 3 -->
        <section class="section">
            <h2>3. Выбор метрики после просмотра данных (p-hacking)</h2>

            <p>
                Соблазн «немного покопаться в данных» после того, как основная метрика не дала результата,
                знаком почти каждому аналитику. Именно так рождаются красивые графики, которых не должно быть
                в отчёте, и фичи, которые запускают по ложноположительным результатам.
            </p>

            <h3>Как выглядит ошибка</h3>
            <p>
                Сценарий:
            </p>
            <ul>
                <li>Запускаем A/B-тест, главная метрика — конверсия в покупку (CR).</li>
                <li>Через 21 день получаем p = 0.12 — незначимо.</li>
                <li>PM спрашивает: «А что по другим метрикам?» — и аналитик смотрит ещё 19 показателей:
                    глубину, время до первой активности, ARPU, скроллинг и т.д.</li>
                <li>Находит метрику «time to first action» с p = 0.04 и выносит её в слайды.</li>
            </ul>

            <p>
                Формально p = 0.04 &lt; 0.05, значит «статистически значимо». Но на самом деле мы сделали 20 независимых
                проверок и выбрали минимальный p-value.
            </p>

            <h3>Почему это проблема</h3>
            <div class="formula">
                P(\text{хотя бы один ложноположительный}) =
                1 - (1 - \alpha)^m
            </div>
            <p>
                При m = 15 и α = 0.05:
            </p>
            <div class="formula">
                1 - 0.95^{15} \approx 54\%
            </div>
            <p>
                То есть вероятность найти хотя бы одну «значимую» метрику при отсутствии эффекта
                больше половины. Это уже не наука, а <em>p-hacking</em>.
            </p>

            <h3>Что делать правильно</h3>
            <ul>
                <li><strong>Фиксировать primary metric до запуска.</strong> В описании эксперимента явно укажите,
                    какая метрика является основной, и именно по ней принимайте решение.</li>
                <li><strong>Вторичные метрики</strong> разрешены, но:
                    <ul>
                        <li>их список должен быть зафиксирован до старта;</li>
                        <li>для них применяйте <strong>исправление на множественные сравнения</strong>:
                            Bonferroni, Holm, Benjamini–Hochberg (BH) и т.д.</li>
                    </ul>
                </li>
                <li><strong>Exploratory анализ</strong> можно делать, но его выводы должны быть явно помечены
                    как гипотезы для будущих тестов, а не как готовые решения.</li>
            </ul>

            <div class="callout callout-warn">
                <p><strong>Опасный сигнал:</strong> если в отчёте нет ни слова про предварительный план анализа,
                    а есть только одна «нашедшаяся» метрика с p ≈ 0.04, скорее всего вы видите результат p-hacking.</p>
            </div>
        </section>

        <!-- Section 4 -->
        <section class="section">
            <h2>4. Multiple testing, peeking и ранняя остановка</h2>

            <p>
                В реальных продуктах редко удаётся «закрыть глаза на 21 день и посмотреть один раз».
                PM-ам хочется знать результаты каждый день, и аналитики начинают смотреть на p-value
                после каждого обновления данных. Это называется <strong>optional stopping</strong> и
                приводит к тем же проблемам, что и множественные тесты.
            </p>

            <h3>Как выглядит ошибка</h3>
            <p>
                План: эксперимент на 21 день. Факты:
            </p>
            <ul>
                <li>С первого дня аналитик строит CR и p-value по кумулятивным данным.</li>
                <li>На 7-й день получает p = 0.03 и радуется.</li>
                <li>PM говорит: «Отлично, ship it» — эксперимент останавливают и выкатывают фичу.</li>
            </ul>
            <p>
                Формально p &lt; 0.05, но было сделано 7 независимых «просмотров» (а часто и больше, если счёт ежедневно).
                Эффективный α становится гораздо выше 5% — исследования показывают, что при ежедневном peeking
                он может вырасти до 15–20%.
            </p>

            <h3>Почему это проблема</h3>
            <ul>
                <li>Каждый дополнительный просмотр — это ещё один тест гипотезы H0: effect = 0.</li>
                <li>«p = 0.03 на 7-й день» <strong>не равно</strong> «значимо на уровне 5%»,
                    если вы не учли факт множественных проверок во времени.</li>
                <li>Вероятность ложноположительного результата <strong>накапливается</strong>
                    по мере того, как вы ждёте «пока p станет красивым».</li>
            </ul>

            <h3>Что делать правильно</h3>
            <ul>
                <li><strong>Фиксированный горизонт.</strong> Самый простой вариант — заранее выбрать длительность теста
                    и честно смотреть на данные один раз в конце. Любые промежуточные графики — только для мониторинга
                    аномалий, а не для принятия решений.</li>
                <li><strong>Sequential testing.</strong> Если хотите иметь право останавливаться раньше, используйте
                    последовательные методы:
                    <ul>
                        <li>классические схемы (O'Brien–Fleming, Pocock);</li>
                        <li><strong>always-valid p-values</strong> и методы типа SPRT;</li>
                        <li>bayesian подходы с контролем FDR.</li>
                    </ul>
                </li>
                <li><strong>Жёсткое правило:</strong> если вы не спроектировали sequential-дизайн заранее,
                    <em>не останавливайте тест раньше</em>, даже если p временно стало &lt; 0.05.</li>
            </ul>
        </section>

        <!-- Section 5 -->
        <section class="section">
            <h2>5. Sample Ratio Mismatch (SRM)</h2>

            <p>
                SRM — это ситуация, когда фактическое распределение трафика по группам A/B
                существенно отличается от ожидаемого. Это ранний сигнал, что в эксперименте
                что-то фундаментально сломано.
            </p>

            <h3>Как выглядит ошибка</h3>
            <p>
                Вы планируете сплит 50/50. Через несколько дней выгружаете данные:
            </p>
            <ul>
                <li>Ожидалось: 500&nbsp;000 / 500&nbsp;000 пользователей.</li>
                <li>Фактически: 503&nbsp;000 в тесте и 497&nbsp;000 в контроле.</li>
            </ul>
            <p>
                Аналитик считает χ²-тест на пропорции и получает p = 0.001 — то есть отклонение
                от 50/50 статистически значимо. Но говорит: «Ну, это же всего 0.3 процентного пункта,
                давайте считать, что всё ок» — и продолжает анализ.
            </p>

            <h3>Почему это проблема</h3>
            <ul>
                <li>SRM почти всегда означает <strong>систематическую проблему</strong>:
                    некорректное кеширование, фильтрация ботов только в одной группе, баг в маршрутизации.</li>
                <li>Если разные типы пользователей с разной вероятностью попадают в тест и контроль,
                    то <strong>оценка эффекта смещена</strong> — мы сравниваем неэквивалентные группы.</li>
                <li>Чем больше выборка, тем легче обнаружить SRM — «маленькие» отклонения по процентам
                    могут быть статистически огромными.</li>
            </ul>

            <h3>Что делать правильно</h3>
            <div class="callout">
                <p><strong>Правило:</strong> прежде чем смотреть на метрики, всегда проверяйте SRM
                    по числу пользователей (или рандомизационных единиц) в группах.</p>
            </div>
            <ul>
                <li>Считайте χ²-тест на соответствие ожидаемому сплиту (50/50, 33/33/33 и т.п.).</li>
                <li>При обнаружении SRM:
                    <ul>
                        <li>остановите интерпретацию результатов — <strong>нельзя доверять эффектам</strong>;</li>
                        <li>разоберите причины: кеширование, redirect, фильтры, баги в bucketing-системе;</li>
                        <li>после исправления проведите новый эксперимент.</li>
                    </ul>
                </li>
            </ul>

            <div class="callout callout-warn">
                <p><strong>Нельзя:</strong> «ну это всего 0.3%, давайте закроем глаза». SRM — это не про величину смещения,
                    а про факт, что рандомизация ведёт себя не так, как должна.</p>
            </div>
        </section>

        <!-- Section 6 -->
        <section class="section">
            <h2>6. Interference / spillover / network effects</h2>

            <p>
                В маркетплейсах и социальных продуктах пользователи связаны друг с другом.
                Treatment, назначенный одному, почти неизбежно влияет на окружение — продавцов,
                друзей, коллег. Это нарушает базовое предположение SUTVA и искажает оценки эффекта.
            </p>

            <h3>Как выглядит ошибка</h3>
            <p>
                Пример marketplace:
            </p>
            <ul>
                <li>Рандомизация по покупателям: половина покупателей получает новый ранжирующий алгоритм.</li>
                <li>Изменяется поведение продавцов: они подстраивают цены и ассортимент под новый трафик.</li>
                <li>Контрольные покупатели также взаимодействуют с этими продавцами и чувствуют эффект treatment,
                    хотя сами формально в контроле.</li>
            </ul>
            <p>
                Пример социальной сети:
            </p>
            <ul>
                <li>Рандомизация по пользователям: новым алгоритмом ленты пользуются только treatment-пользователи.</li>
                <li>Они чаще лайкают и делятся контентом, который видят их друзья из контроля.</li>
                <li>Контрольные пользователи начинают вести себя иначе, хотя фичу им не включали.</li>
            </ul>

            <h3>Почему это проблема</h3>
            <ul>
                <li>Часть эффекта «просачивается» в контроль → <strong>оценка эффекта занижена</strong>
                    (attenuation towards zero).</li>
                <li>При полном rollout структура сети меняется, и реальный эффект может быть
                    существенно выше (или ниже), чем в эксперименте.</li>
                <li>Стандартные методы анализа для независимых единиц (пользователей) больше не валидны.</li>
            </ul>

            <h3>Что делать правильно</h3>
            <ul>
                <li><strong>Кластерная рандомизация.</strong> Рандомизируйте целые кластеры графа:
                    города, школы, группы друзей, компоненты сообщества.</li>
                <li><strong>Switchback-дизайны.</strong> Для временных эффектов (логистика, supply-side)
                    можно чередовать treatment и контроль по времени для одного и того же кластера.</li>
                <li><strong>Моделирование экспозиции.</strong> Оценивайте, как метрика в контроле зависит
                    от доли соседей в treatment; это поможет понять масштаб spillover.</li>
            </ul>

            <div class="callout">
                <p><strong>Связь с симуляторами:</strong> <strong>Cluster Simulator</strong> помогает увидеть,
                    как нарушение независимости наблюдений и кластерные эффекты раздувают ошибку первого рода
                    при наивном анализе.</p>
            </div>
        </section>

        <!-- Section 7 -->
        <section class="section">
            <h2>7. Поломанный знаменатель (treatment-affected exposure)</h2>

            <p>
                Большинство пропорций и rate-метрик имеют вид «события / экспозиция» — CTR, CR, RPM, retention.
                Ключевая ошибка — когда treatment изменяет не только числитель (clicks), но и знаменатель (impressions),
                а аналитик продолжает интерпретировать результат как «чистый» эффект на отношение.
            </p>

            <h3>Как выглядит ошибка</h3>
            <p>
                Тестируется новый алгоритм таргетинга рекламы:
            </p>
            <ul>
                <li>В тесте на 30% больше показов на пользователя (алгоритм находит больше инвентаря).</li>
                <li>Общее число кликов тоже растёт, но не пропорционально показам.</li>
                <li>CTR = clicks / impressions падает с 2.1% до 1.8%.</li>
                <li>PM смотрит только на CTR и говорит: «Алгоритм ухудшил эффективность, отменяем».</li>
            </ul>

            <h3>Почему это проблема</h3>
            <ul>
                <li>Знаменатель (impressions) теперь <strong>зависит от treatment</strong>,
                    поэтому CTR смешивает два эффекта: изменение стратегии показа и изменение отклика пользователей.</li>
                <li>Может оказаться, что общее число кликов или revenue на пользователя выросло,
                    но CTR упал из-за роста низокачественных показов.</li>
                <li>В таком случае говорить «фича ухудшила CTR, значит она плохая» — некорректно.</li>
            </ul>

            <h3>Что делать правильно</h3>
            <ul>
                <li>Проверять <strong>стабильность знаменателя</strong> между группами: impressions per user,
                    sessions per user, number of opportunities.</li>
                <li>Если denominators существенно различаются:
                    <ul>
                        <li>перейти к <strong>per-user метрикам</strong> (клики на пользователя, выручка на пользователя,
                            время на пользователя);</li>
                        <li>использовать ITT-подход: считать эффект на всех рандомизированных пользователях,
                            включая тех, кто вовсе не получил экспозицию;</li>
                        <li>интерпретировать CTR/CR только как вспомогательные диагностические метрики.</li>
                    </ul>
                </li>
            </ul>

            <div class="callout">
                <p><strong>Связь с симуляторами:</strong> в <strong>карте выбора статкритерия</strong>
                    можно задать метрику как ratio с treatment-affected denominator и посмотреть, какие тесты
                    рекомендуются, а какие приводят к смещению.</p>
            </div>
        </section>

        <!-- Preview sections -->
        <section class="preview-section">
            <h2>Практика</h2>
            <ul class="preview-list">
                <li>14 задач на fan-out, SRM, p-hacking и поломанные ratio-метрики</li>
                <li>Разбор многократных тестов, ранней остановки и interference</li>
                <li>Практические советы по дизайну экспериментов и чек-листы для ревью</li>
            </ul>
            <a href="./practice.html" class="nav-card">Открыть практику</a>
        </section>

        <section class="preview-section">
            <h2>Симуляторы</h2>
            <ul class="preview-list">
                <li>Карта выбора статкритерия для сложных метрик и дизайнов</li>
                <li>Классификатор метрик, помогающий избежать ошибок единицы анализа</li>
                <li>Cluster Simulator, показывающий последствия interference и кластеризации</li>
            </ul>
            <a href="./simulators.html" class="nav-card">Открыть симуляторы</a>
        </section>

        <nav class="module-nav">
            <a href="../06_cluster_tests/index.html">← Предыдущий модуль</a>
            <a href="../../index.html">К курсу</a>
            <a href="../08_quasi_experiments/index.html">Следующий модуль →</a>
        </nav>
    </div>
</body>
</html>

