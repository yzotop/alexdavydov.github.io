<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Практика — Модуль 7. Типичные ошибки</title>
    <link rel="stylesheet" href="../../assets/style.css">
</head>
<body>
    <div class="container">
        <div class="breadcrumbs">
            <a href="../../../../courses/index.html">Все курсы</a> /
            <a href="../../index.html">Статистика A/B-тестирования</a> /
            <a href="./index.html">Модуль 7</a> / Практика
        </div>

        <div class="nav-links">
            <a href="./index.html">Модуль</a>
            <a href="./practice.html" class="active">Практика</a>
            <a href="./simulators.html">Симуляторы</a>
        </div>

        <h1>Практика</h1>
        <p class="subtitle">Для каждой задачи определите, какая именно ошибка допущена, как её диагностировать по данным и логам эксперимента и как скорректировать дизайн или анализ.</p>

        <!-- Task 1 -->
        <div class="task-card">
            <div class="task-title">Задача 1. Fan-out в event-level анализе</div>
            <div class="task-body">
                <p>Команда запускает A/B-тест новой воронки checkout. Рандомизация — по <code>user_id</code>, 50/50.</p>
                <p>Аналитик выбирает метрику «доля сессий с покупкой» и считает t-test на уровне сессий. N получается 8&nbsp;млн сессий, p = 0.02.</p>
                <p>В логах видно, что есть небольшая группа power users, у которых по 50–100 сессий за период, и они чаще попали в тест.</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Что сломано:</strong> ошибка единицы анализа и fan-out. Эксперимент рандомизируется по пользователям, а анализ ведётся по сессиям. Power users с десятками сессий получают непропорционально большой вес.</p>
                    <p><strong>Как проверить:</strong> построить распределение числа сессий на пользователя по группам, посмотреть долю total сессий, приходящихся на топ-1% пользователей; сравнить результаты t-test на уровне сессий и на уровне пользователей (одна строка на user_id).</p>
                    <p><strong>Как исправить:</strong> агрегировать данные до user-level (метрика: «есть покупка / нет покупки у пользователя за период») и пересчитать эффект. При необходимости дополнительно ограничить вклад экстремальных пользователей (winsorization или отсечение по числу сессий).</p>
                </div>
            </details>
        </div>

        <!-- Task 2 -->
        <div class="task-card">
            <div class="task-title">Задача 2. AOV через mean of ratios</div>
            <div class="task-body">
                <p>Аналитик хочет оценить влияние новой рекомендательной ленты на средний чек (AOV).</p>
                <p>Он считает для каждого пользователя AOV<sub>i</sub> = GMV<sub>i</sub> / Orders<sub>i</sub>, затем берёт среднее по пользователям и сравнивает группы простым t-test по этим значениям.</p>
                <p>В примере из выборки есть пользователь A с 1 заказом на 50&nbsp;₽ и пользователь B со 100 заказами, суммарно на 50&nbsp;000&nbsp;₽ — оба дают одинаковый вес в среднем AOV.</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Что сломано:</strong> используется mean of ratios вместо ratio of means, heavy users недооценены. Оценка AOV смещена относительно того, что обычно понимается как «средний чек заказа».</p>
                    <p><strong>Как проверить:</strong> посчитать AOV как глобальное отношение суммарного GMV к суммарному числу заказов в каждой группе и сравнить с mean of ratios. Если результаты сильно различаются, значит mean of ratios искажает картину.</p>
                    <p><strong>Как исправить:</strong> использовать ratio of means: считать AOV<sub>group</sub> = ΣGMV / ΣOrders и применять к нему delta-method или bootstrap по пользователям для оценки дисперсии и доверительных интервалов.</p>
                </div>
            </details>
        </div>

        <!-- Task 3 -->
        <div class="task-card">
            <div class="task-title">Задача 3. p-hacking через 20 метрик</div>
            <div class="task-body">
                <p>В эксперименте по новой онбординг-воронке основная метрика — конверсия в первую покупку (CR). По итогам 21 дня CR даёт p = 0.12.</p>
                <p>Продакт просит «посмотреть, что ещё изменилось». Аналитик проверяет ещё 19 метрик: активацию, retention, глубину, engagement, и находит «time to first action» с p = 0.04.</p>
                <p>В презентации в итоге фигурирует только эта одна «успешная» метрика, без упоминания остальных 19.</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Что сломано:</strong> множественное тестирование без коррекции и выбор метрики после просмотра данных (p-hacking). При 20 метриках и α = 0.05 ожидается ~1 ложноположительный результат даже при нулевом эффекте.</p>
                    <p><strong>Как проверить:</strong> посчитать количество проверенных метрик и распределение их p-value; оценить вероятность хотя бы одного ложноположительного: 1 − (1 − 0.05)<sup>20</sup>. Проверить, была ли заранее задокументирована primary metric и список вторичных.</p>
                    <p><strong>Как исправить:</strong> явно зафиксировать primary metric до запуска и принимать решение только по ней. Для вторичных метрик применять коррекцию (Bonferroni, Holm, BH). Анализ «19 дополнительных метрик» считать exploratory и использовать только для генерации гипотез для будущих тестов.</p>
                </div>
            </details>
        </div>

        <!-- Task 4 -->
        <div class="task-card">
            <div class="task-title">Задача 4. Ранняя остановка без sequential design</div>
            <div class="task-body">
                <p>Эксперимент запланирован на 28 дней. С первого дня PM просит ежедневный отчёт по CR и p-value.</p>
                <p>На 10-й день аналитик докладывает: «p = 0.04, кажется, победили». PM решает завершить эксперимент и выкатывать фичу, не дожидаясь оставшихся 18 дней.</p>
                <p>Постфактум, если бы эксперимент довели до 28 дней, оказалось бы, что итоговое p = 0.18 и эффект нестабилен.</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Что сломано:</strong> multiple testing по времени (peeking) и ранняя остановка без заранее спроектированного sequential-дизайна. p = 0.04 на 10-й день нельзя интерпретировать как «значимо на 5%».</p>
                    <p><strong>Как проверить:</strong> посчитать количество фактических «просмотров» p-value (по дням или по батчам данных); смоделировать (или оценить теоретически) рост эффективного α при таком числе промежуточных проверок.</p>
                    <p><strong>Как исправить:</strong> либо придерживаться фиксированного горизонта (решение только в конце), либо заранее спроектировать sequential-test (O'Brien–Fleming, always-valid p-values) с формальными правилами остановки и использовать соответствующие границы для p-value.</p>
                </div>
            </details>
        </div>

        <!-- Task 5 -->
        <div class="task-card">
            <div class="task-title">Задача 5. SRM обнаружен, но проигнорирован</div>
            <div class="task-body">
                <p>Запущен эксперимент с ожидаемым сплитом 50/50, целевая аудитория — 1&nbsp;млн пользователей.</p>
                <p>По факту в выгрузке: 503&nbsp;000 пользователей в тесте и 497&nbsp;000 в контроле. χ²-тест на соответствие 50/50 даёт p = 0.001.</p>
                <p>Аналитик говорит: «Разница всего 0.3 п.п., давайте не будем придираться» и продолжает анализ CR и AOV.</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Что сломано:</strong> игнорирование Sample Ratio Mismatch. Значимое отклонение от запланированного сплита указывает на систематическую проблему в рандомизации или отборе данных.</p>
                    <p><strong>Как проверить:</strong> формально провести χ²-тест для всех активных экспериментов; сегментировать пользователей по ключевым признакам (страна, платформа, источник трафика) и проверить, нет ли дисбаланса сплита внутри сегментов.</p>
                    <p><strong>Как исправить:</strong> остановить интерпретацию эффекта, исследовать причины SRM (кеши, редиректы, баги в bucketing, фильтрация ботов только в одной группе). После исправления — перезапустить эксперимент. Не использовать текущие результаты для принятия решений.</p>
                </div>
            </details>
        </div>

        <!-- Task 6 -->
        <div class="task-card">
            <div class="task-title">Задача 6. Interference в социальной сети</div>
            <div class="task-body">
                <p>Социальная сеть тестирует новый формат кнопки «Поделиться». Рандомизация по пользователям: 50% получают новую кнопку, 50% остаются со старой.</p>
                <p>Пользователи из теста начинают делиться контентом чаще, этот контент попадает в ленты их друзей, включая тех, кто остался в контроле.</p>
                <p>По итогам эксперимента аналитик видит +3% к просмотрам ленты у тестовых пользователей и +1.5% у контрольных, и делает вывод, что «эффект небольшой».</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Что сломано:</strong> сильный spillover между treatment и control через социальный граф, нарушение SUTVA. Контрольная группа частично экспонирована эффекту treatment.</p>
                    <p><strong>Как проверить:</strong> проанализировать зависимость метрик контрольных пользователей от доли их друзей в тесте; посмотреть, не растут ли просмотры в контроле сильнее именно у тех, у кого много «тестовых» друзей.</p>
                    <p><strong>Как исправить:</strong> перейти к кластерной рандомизации по кластерам графа (сообщества, школы, компании) или использовать специализированные network-экспериментальные дизайны; интерпретировать текущий +3% как нижнюю оценку эффекта при полном rollout.</p>
                </div>
            </details>
        </div>

        <!-- Task 7 -->
        <div class="task-card">
            <div class="task-title">Задача 7. Treatment-affected exposure (CTR)</div>
            <div class="task-body">
                <p>Тестируется новый алгоритм таргетинга рекламы. В тестовой группе количество показов на пользователя выросло на 30%, а общее число кликов увеличилось на 10%.</p>
                <p>CTR упал с 2.1% до 1.8%. PM делает вывод: «реклама стала менее эффективной, отключаем алгоритм».</p>
                <p>Аналитик сомневается: абсолютное число кликов и выручка от рекламы на пользователя всё же выросли.</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Что сломано:</strong> поломанный знаменатель. Treatment изменяет количество показов (denominator), и CTR уже не отражает «чистый» отклик пользователей.</p>
                    <p><strong>Как проверить:</strong> сравнить между группами не только CTR, но и клики на пользователя, выручку на пользователя и распределение impressions per user; проверить, нет ли сильного смещения в сторону «дешёвых» показов.</p>
                    <p><strong>Как исправить:</strong> фокусировать выводы на per-user метриках (клики/выручка на пользователя), использовать ITT-подход (все рандомизированные пользователи, даже с нулевой экспозицией). CTR оставить как диагностическую метрику, но не как основную для решения.</p>
                </div>
            </details>
        </div>

        <!-- Task 8 -->
        <div class="task-card">
            <div class="task-title">Задача 8. Survivors bias в retention</div>
            <div class="task-body">
                <p>Проводится эксперимент с новым онбордингом. Через 7 дней измеряется Retention D7 для тех, кто прошёл активацию (совершил целевое действие в день 0).</p>
                <p>В тесте: D7 среди активированных = 42%, в контроле = 38%. Но при этом доля пользователей, которые вообще дошли до активации, в тесте на 5 п.п. ниже, чем в контроле.</p>
                <p>В отчёте показывают только D7 по активированным и заявляют, что retention улучшился.</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Что сломано:</strong> выживший (survivor) bias и условная метрика. Сравниваются только те, кто прошёл через воронку, хотя сама вероятность пройти в тесте ниже.</p>
                    <p><strong>Как проверить:</strong> посмотреть конверсию в активацию по группам; посчитать ITT-метрику «доля всех рандомизированных пользователей, вернувшихся на D7», а не только среди активированных.</p>
                    <p><strong>Как исправить:</strong> использовать ITT-подход по всему когорту; анализировать две вещи: (1) влияние на активацию, (2) влияние на retention среди активированных, и интерпретировать их совместно, а не по отдельности.</p>
                </div>
            </details>
        </div>

        <!-- Task 9 -->
        <div class="task-card">
            <div class="task-title">Задача 9. Post-treatment сегментация</div>
            <div class="task-body">
                <p>После эксперимента по новой ленте аналитик решает «поглубже покопаться» и делит пользователей на сегменты по активности во время теста: high-activity (≥ 20 сессий) и low-activity (&lt; 20 сессий).</p>
                <p>В high-activity сегменте находит +10% CR и p = 0.01, в low-activity сегменте — ничего значимого. В отчёте делается акцент именно на high-activity.</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Что сломано:</strong> сегментация по post-treatment признаку, который сам может зависеть от treatment. Активность во время эксперимента — уже результат фичи.</p>
                    <p><strong>Как проверить:</strong> сравнить распределение активности между группами; проверить, нет ли сильного роста доли high-activity пользователей именно в тесте (что и является эффектом treatment).</p>
                    <p><strong>Как исправить:</strong> использовать предэкспериментальные признаки для сегментации (активность за прошлый месяц, LTV, канал трафика). Post-treatment сегменты можно использовать только для генерации гипотез и визуализаций, но не для формальных выводов.</p>
                </div>
            </details>
        </div>

        <!-- Task 10 -->
        <div class="task-card">
            <div class="task-title">Задача 10. Ratio-метрика + кластерная рандомизация</div>
            <div class="task-body">
                <p>Проводится geo-эксперимент по 12 городам (6 тест, 6 контроль). Метрика — RPM (revenue per 1000 impressions).</p>
                <p>Аналитик выгружает данные на уровне пользователей, считает RPM<sub>i</sub> = Revenue<sub>i</sub> / Impressions<sub>i</sub> и запускает обычный t-test по 600&nbsp;тыс. пользователей.</p>
                <p>В анализе никак не учитывается ни ratio-природа метрики, ни факт, что рандомизация была по городам.</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Что сломано:</strong> двойная ошибка. (1) RPM — ratio-метрика, для неё нужен ratio of means + специальный анализ (delta-method, bootstrap). (2) Рандомизация по городам → единица анализа должна быть кластером, а не пользователем.</p>
                    <p><strong>Как проверить:</strong> посчитать RPM как ΣRevenue / ΣImpressions на уровне городов; сравнить результаты user-level и city-level анализа; оценить, насколько сильно различаются p-value.</p>
                    <p><strong>Как исправить:</strong> агрегировать данные до уровня города: для каждого города посчитать ΣRevenue и ΣImpressions, взять ratio и затем провести тест по 12 наблюдениям (t-test или permutation test). Либо использовать регрессию с cluster-robust SE по городам и delta-method для ratio.</p>
                </div>
            </details>
        </div>

        <!-- Task 11 -->
        <div class="task-card">
            <div class="task-title">Задача 11. Novelty effect</div>
            <div class="task-body">
                <p>Новая версия UI ленты запускается на 50% пользователей. В первую неделю среднее время в приложении у тестовой группы на 15% выше, чем у контроля, p &lt; 0.01.</p>
                <p>К концу четвёртой недели uplift снижается до +2%, а на пятой неделе почти исчезает. Аналитик всё равно в отчёте показывает график за первую неделю и делает акцент на +15%.</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Что сломано:</strong> игнорирование novelty / learning effect. Пользователи активно исследуют новый интерфейс, но долгосрочное поведение оказывается гораздо ближе к контролю.</p>
                    <p><strong>Как проверить:</strong> построить динамику эффекта по неделям; сравнить краткосрочный uplift с устойчивым уровнем на 3–4 неделе; проверить, не изменяется ли распределение по когортам (новые vs старые пользователи).</p>
                    <p><strong>Как исправить:</strong> фокусироваться на стабилизированном периоде (последние недели эксперимента) или использовать долгоживущие holdout-группы; в отчётах явно разделять краткосрочный novelty-эффект и долгосрочный steady-state.</p>
                </div>
            </details>
        </div>

        <!-- Task 12 -->
        <div class="task-card">
            <div class="task-title">Задача 12. SRM из-за кеширования</div>
            <div class="task-body">
                <p>A/B-сплит реализован на сервере: при первом заходе пользователя ему назначается variant, и дальше он должен получать соответствующую версию страницы.</p>
                <p>Однако перед фронтендом стоит CDN, который кеширует HTML по URL без учёта куки с variant. Часть пользователей, формально попавших в контроль, видит кешированную версию теста.</p>
                <p>В статистике видно SRM: тест = 52%, контроль = 48%; при этом по логам CDNs понятно, что кеш пробивается неоднородно по странам и девайсам.</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Что сломано:</strong> кеширование обходит механизм рандомизации, пользователи видят не тот вариант, который им назначен. Это приводит и к SRM, и к нарушению чистоты контроля.</p>
                    <p><strong>Как проверить:</strong> сравнить variant по логам бэкенда и фактически отданную версию (по шаблонам, версиям CSS/JS); посмотреть распределение variant vs фактической разметки по странам/устройствам/POP-узлам.</p>
                    <p><strong>Как исправить:</strong> настроить CDN так, чтобы в ключ кеша входили кука/заголовок с variant; либо отключить кеширование для экспериментальных URL; после исправления — перезапустить эксперимент, не используя текущие данные для выводов.</p>
                </div>
            </details>
        </div>

        <!-- Task 13 -->
        <div class="task-card">
            <div class="task-title">Задача 13. Multiple comparisons в A/B/C/D тесте</div>
            <div class="task-body">
                <p>Проводится эксперимент с 4 вариантами интерфейса: A (контроль), B, C, D. Цель — найти лучший из трёх новых вариантов.</p>
                <p>Аналитик считает все 6 попарных сравнений (A-B, A-C, A-D, B-C, B-D, C-D) и находит одно значимое: B выигрывает у C с p = 0.04.</p>
                <p>В презентации делается вывод: «вариант B статистически лучше варианта C, поэтому его и выкатим».</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Что сломано:</strong> множественные сравнения без коррекции. При 6 сравнениях вероятность хотя бы одного ложноположительного результата при α = 0.05 существенно выше 5%.</p>
                    <p><strong>Как проверить:</strong> зафиксировать, какие именно сравнения планировались (только vs контроль или все попарные); оценить ожидаемое число ложноположительных: 6 × 0.05 = 0.3; проверить устойчивость результата при коррекциях.</p>
                    <p><strong>Как исправить:</strong> если интересуют только сравнения с контролем — использовать методы типа Dunnett или Holm. Если все попарные — применять Bonferroni/Holm/BH-исправления и/или многофакторные модели (ANOVA с post-hoc тестами), а не делать выводы по одиночному p = 0.04 без коррекции.</p>
                </div>
            </details>
        </div>

        <!-- Task 14 -->
        <div class="task-card">
            <div class="task-title">Задача 14. Conditional метрика без учёта композиции</div>
            <div class="task-body">
                <p>Новая акция увеличила конверсию в покупку на 20%. При этом ARPPU (average revenue per paying user) в тестовой группе снизился на 8% относительно контроля.</p>
                <p>Аналитик делает вывод: «акция ухудшает монетизацию на покупателя, не будем её запускать».</p>
                <p>Однако видно, что в тесте появилось много новых «лёгких» покупателей с небольшими чеками, которых в контроле почти не было.</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Что сломано:</strong> неправильная интерпретация conditional-метрики. ARPPU считается только по платящим пользователям, а treatment изменяет саму популяцию платящих (композиционный эффект).</p>
                    <p><strong>Как проверить:</strong> сравнить распределение чеков и число платящих пользователей в группах; посчитать ITT-метрику «общая выручка на пользователя» и посмотреть, как она изменилась.</p>
                    <p><strong>Как исправить:</strong> для принятия решений использовать ITT-метрики (выручка на пользователя, GMV на пользователя), а ARPPU трактовать как вспомогательную; при желании можно применить двухэтапный анализ: сначала эффект на конверсию в покупку, затем — на средний чек при фиксированной группе покупателей (например, по когорте уже плативших до эксперимента).</p>
                </div>
            </details>
        </div>

        <nav class="module-nav">
            <a href="../06_cluster_tests/index.html">← Предыдущий модуль</a>
            <a href="../../index.html">К курсу</a>
            <a href="../08_quasi_experiments/index.html">Следующий модуль →</a>
        </nav>
    </div>
</body>
</html>

