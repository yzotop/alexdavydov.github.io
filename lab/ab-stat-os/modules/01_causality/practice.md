# Практика — Каузальность

Для каждой задачи определите:
- Какое условие каузальности нарушено (если нарушено)
- Почему это проблема
- Что нужно сделать

Ответы — после каждой задачи.

---

## Задача 1. Рандомизация по времени регистрации

Команда запускает новый онбординг. Все пользователи, зарегистрированные до 1 марта — контроль (старый онбординг). Все зарегистрированные после 1 марта — тест (новый онбординг). Через месяц сравнивают retention.

### Ответ

**Нарушено:** рандомизация. Это не A/B-тест, а before/after сравнение. Группы различаются по времени регистрации — любой внешний фактор (маркетинговая кампания, сезонность, изменение трафика) confounds результат. Разница в retention может быть вызвана не онбордингом, а составом аудитории в марте vs феврале.

**Что делать:** рандомизировать по user_id — каждый новый пользователь с равной вероятностью получает старый или новый онбординг, независимо от даты.

---

## Задача 2. Маркетплейс: новый алгоритм ранжирования

E-commerce маркетплейс тестирует новый алгоритм ранжирования товаров. Рандомизация по покупателям: 50% видят новый алгоритм, 50% — старый. Метрика — GMV per buyer. Через 2 недели тест показывает +8% GMV в тест-группе.

Но: новый алгоритм показывает товары определённых продавцов чаще. Эти продавцы получают больше продаж, а продавцы, чьи товары ушли ниже — меньше.

### Ответ

**Нарушено:** SUTVA (отсутствие интерференции). Treatment для одних покупателей влияет на товарное предложение для других. Продавцы — общий ресурс: если один покупатель видит товар выше, другой может не увидеть его вовсе (finite inventory, shared attention).

**Следствие:** +8% GMV в тесте частично достигнуто за счёт контроля. Реальный ATE при 100% rollout будет значительно ниже — возможно, близок к нулю (zero-sum redistribution).

**Что делать:** (1) Рандомизация по кластерам (города, регионы) — если marketplace localized. (2) Мониторить метрики продавцов — не просели ли продавцы, обслуживающие контроль. (3) При rollout сравнить фактический GMV с прогнозом из A/B.

---

## Задача 3. CTR нового рекламного формата

Команда тестирует новый формат рекламного баннера. Рандомизация по пользователям. Метрика: CTR = клики / показы (глобальное отношение). Результат: CTR вырос на +12%.

Но: новый формат занимает больше экранного пространства, из-за чего алгоритм показывает *меньше* баннеров на пользователя (было 8, стало 5).

### Ответ

**Проблема:** treatment-affected exposure. Treatment изменил знаменатель метрики (число показов). CTR вырос не потому, что баннер лучше, а потому, что число показов сократилось. Если раньше было 8 показов и 1 клик (CTR = 12.5%), а теперь 5 показов и 1 клик (CTR = 20%) — клики не изменились, но CTR «вырос».

**Следствие:** наивный CTR некорректно отражает эффект. Метрика confounded через знаменатель.

**Что делать:** (1) Сравнить абсолютное число кликов per user (не ratio). (2) Сравнить число показов между группами. (3) Если показы различаются — CTR невалиден как primary metric.

---

## Задача 4. ARPPU и новый онбординг

Эксперимент: новый онбординг увеличивает конверсию в первую покупку. Метрика для оценки эффекта — ARPPU (средняя выручка на покупателя, то есть только среди пользователей, совершивших покупку).

Результат: ARPPU в тест-группе на 15% *ниже*, чем в контроле. Вывод аналитика: «новый онбординг ухудшает выручку с покупателя».

### Ответ

**Нарушено:** post-treatment conditioning. ARPPU считается только среди покупателей — это подмножество, определённое post-treatment поведением. Treatment увеличил конверсию → в тесте больше «новых» покупателей с мелкими чеками → средний чек среди покупателей падает.

**Следствие:** ARPPU ↓ не означает, что treatment плох. Total revenue мог вырасти (больше покупателей × меньший средний чек > меньше покупателей × больший средний чек).

**Что делать:** использовать ITT-метрику (revenue per user, включая нули) как primary. ARPPU — вспомогательная метрика для диагностики, не для принятия решения.

---

## Задача 5. Гео-эксперимент с ценами

Компания тестирует повышение цен в приложении доставки еды. Рандомизация по городам: 10 городов — тест, 10 — контроль. Метрика: средний чек.

Аналитик проводит t-test на уровне пользователей (500K в тесте, 480K в контроле). Получает p = 0.0003. Вывод: «повышение цен значимо увеличило средний чек».

### Ответ

**Проблема:** неправильная единица анализа. Рандомизация — по городам (кластерам), а анализ — по пользователям. Пользователи внутри одного города скоррелированы (одинаковые цены, одинаковые рестораны, одинаковая погода). Эффективный N ≈ 20 (число городов), а не 980K.

**Следствие:** t-test с N = 980K драматически занижает p-value. Реальная значимость при кластерном анализе: p ≈ 0.15–0.35. Мощность эксперимента ~20%.

**Что делать:** cluster-robust SE с кластером = город. Или randomization inference (permutation по 20 городам). Принять, что N_eff = 20 и power низкая.

---

## Задача 6. Утечка через shared device

Эксперимент: новый UI checkout-страницы. Рандомизация по user_id. Семейный аккаунт: несколько членов семьи используют один аккаунт. Один видит новый UI, другой — тоже (потому что тот же user_id).

Но: часть пользователей контрольной группы обсуждают «странную новую страницу оплаты» в семейном чате (screenshot от пользователя из теста).

### Ответ

**Нарушено:** (1) Консистентность — если shared device, то один user_id может означать разных людей с разным поведением. (2) SUTVA — информация о treatment «протекает» в контроль через социальные связи.

**Следствие:** контроль загрязнён. Если пользователи контроля узнают о новом UI и меняют поведение, ATE занижен (diluted).

**Что делать:** (1) Рандомизация по устройству или cookie, а не user_id. (2) Анализировать возможный leakage: есть ли кластеры пользователей с общими device_id? (3) Если leakage значительный — кластерная рандомизация по household.

---

## Задача 7. Социальная сеть: новая фича шеринга

Социальная сеть тестирует кнопку «Поделиться» в новом формате. Рандомизация по пользователям. Метрика: число shares per user. Тест-группа: +25% shares.

Но: когда пользователь тест-группы делится контентом, его друзья (возможно из контроля) видят этот контент и тоже начинают делиться чаще.

### Ответ

**Нарушено:** SUTVA. Социальный граф создаёт интерференцию: treatment одного пользователя влияет на поведение его друзей, независимо от их group assignment.

**Следствие:** +25% — переоценка каузального эффекта. Часть роста в тесте вызвана spillover от других пользователей теста. А контроль «загрязнён» обратным spillover.

**Что делать:** (1) Graph cluster randomization — рандомизировать связанные компоненты или communities. (2) Ego-network randomization. (3) Как minimum — замерить степень «загрязнения» контроля: среди пользователей контроля с >50% друзей в тесте shares выше, чем среди тех, у кого мало друзей в тесте?

---

## Задача 8. Эксперимент с багом в delivery

Тест: новый алгоритм распределения заказов по курьерам. Рандомизация по заказам. Метрика: время доставки. Из-за бага 20% заказов в тест-группе обрабатывались старым алгоритмом.

Результат: среднее время доставки в тесте на 3% лучше, чем в контроле.

### Ответ

**Нарушено:** консистентность. 20% тестовой группы фактически получили контрольный treatment. Наблюдаемый Y для них — Y(0), а не Y(1), но мы анализируем их как Y(1).

**Следствие:** attenuation bias — реальный эффект нового алгоритма больше, чем 3%. Если 80% получили treatment, а 20% — контроль, то ITT = 3%, а LATE (Local Average Treatment Effect) = 3% / 0.8 = 3.75%.

**Что делать:** (1) Исправить баг и перезапустить. (2) Если перезапуск невозможен — оценить LATE через IV (instrumental variable): assignment as instrument. (3) Как minimum — раскрыть процент non-compliance в отчёте.

---

## Задача 9. Рандомизация по сессиям

Эксперимент: новый дизайн страницы каталога. Рандомизация по сессиям: каждая сессия случайно попадает в тест или контроль. Один пользователь в разных сессиях может видеть разные версии.

Метрика: конверсия (покупка / пользователь).

### Ответ

**Проблема:** единица рандомизации (сессия) ≠ единица анализа (пользователь). Один пользователь может быть и в тесте, и в контроле — это нарушает assignment consistency и загрязняет обе группы.

**Следствие:** (1) Attenuation bias — эффект размывается. (2) Пользовательский опыт непоследовательный — что само по себе может влиять на поведение (novelty/confusion effect). (3) Нельзя однозначно атрибутировать покупку конкретной версии.

**Что делать:** рандомизировать по user_id (sticky assignment). Если нужна session-level рандомизация — метрика тоже должна быть session-level (actions per session), а не user-level.

---

## Задача 10. AB-тест ценового плана (B2B)

B2B SaaS: тестируют новый ценовой план. Рандомизация по компаниям (100 компаний в тесте, 100 в контроле). Метрика: revenue per company. Результат: +22% revenue в тесте.

Но: 3 крупных enterprise-клиента, попавших в тест, сгенерировали 60% revenue тестовой группы.

### Ответ

**Проблема:** (1) Малая выборка (200 компаний). (2) Экстремальный heavy tail — 3 компании определяют результат. (3) Рандомизация формально корректна, но при таком heavy tail баланс по размеру компаний случаен и ненадёжен.

**Следствие:** +22% может быть целиком объяснён тем, что крупные компании случайно попали в тест. Повторная рандомизация может дать −15%.

**Что делать:** (1) Стратификация по размеру компании при рандомизации (block randomization). (2) Winsorization / trimmed mean. (3) Permutation test вместо t-test. (4) Проверить: какой результат без 3 крупнейших компаний?

---

## Задача 11. Conditioning on a collider

Эксперимент: новая модель рекомендаций. Рандомизация по пользователям. Аналитик анализирует конверсию *только среди пользователей, просмотревших ≥5 товаров* (аргумент: «неактивные пользователи зашумляют результат»).

Treatment влияет на то, сколько товаров пользователь просматривает (рекомендации стали более релевантными → пользователи смотрят больше).

### Ответ

**Нарушено:** post-treatment conditioning. «Просмотр ≥5 товаров» — это post-treatment переменная, зависящая от treatment. Фильтруя по ней, аналитик создаёт разный состав подвыборок в тесте и контроле.

**Следствие:** в тесте подвыборка «≥5 просмотров» включает пользователей, которые *благодаря treatment* стали смотреть больше. В контроле — только изначально активных. Это два разных набора людей. Сравнение их конверсии смещено.

**Что делать:** (1) Анализировать всех пользователей (ITT). (2) Если нужна сегментация — сегментировать по pre-treatment характеристикам (активность за прошлую неделю), а не по post-treatment поведению. (3) Количество просмотренных товаров → secondary metric, а не фильтр.

---

## Задача 12. Два эксперимента одновременно

Две команды одновременно запускают эксперименты на одних и тех же пользователях. Команда A тестирует новый checkout. Команда B тестирует новую цену. Рандомизация независимая (каждый пользователь может быть в любой комбинации: AA, AB, BA, BB).

Команда A видит +5% CR. Но в подгруппе, где одновременно активен тест B (новая цена) — CR +12%. В подгруппе без теста B — CR +1%.

### Ответ

**Проблема:** interaction effect. Два treatment взаимодействуют: новый checkout хорошо работает с новой ценой, но плохо — со старой. Средний эффект (+5%) маскирует гетерогенность.

**Следствие:** если команда A решит запускать новый checkout, а команда B не запустит новую цену — реальный эффект будет +1%, а не +5%.

**Что делать:** (1) Анализировать factorial design (2×2) — смотреть на interaction term. (2) Принимать решение о запуске с учётом того, запустится ли второй эксперимент. (3) При сильном interaction — запускать последовательно или как один эксперимент.

---

## Задача 13. Survivor bias в retention

Эксперимент: новая фича повышает retention. Аналитик измеряет Day 30 retention. Но: в тест-группе 5% пользователей деинсталлировали приложение на Day 3 (из-за багов в новой фиче). Эти пользователи не учитываются в Day 30 retention (они уже «ушли»).

Результат: Day 30 retention в тесте = 42%, в контроле = 38%. Вывод: «фича улучшает retention».

### Ответ

**Нарушено:** survivor bias (форма post-treatment conditioning). Пользователи, ушедшие из-за бага, исключены из анализа. Оставшиеся в тесте — «выжившие», которые пережили баг. Они изначально более лояльны → retention выше.

**Следствие:** 42% retention — это retention *среди выживших*, а не среди всех. Если включить ушедших (retention = 0), реальный Day 30 retention теста = 42% × 0.95 = 39.9% — практически как в контроле.

**Что делать:** (1) ITT-анализ: считать retention по всем пользователям, включая деинсталлировавших (retention = 0). (2) Мониторить uninstall rate как guardrail metric. (3) Если uninstall rate в тесте значимо выше — это red flag, даже если retention «растёт».
