<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Практика — Модуль 1. Каузальность</title>
    <link rel="stylesheet" href="../../assets/style.css">
</head>
<body>
    <div class="container">
        <div class="breadcrumbs">
            <a href="../../../../courses/index.html">Все курсы</a> /
            <a href="../../index.html">Статистика A/B-тестирования</a> /
            <a href="./index.html">Модуль 1</a> / Практика
        </div>

        <div class="nav-links">
            <a href="./index.html">Модуль</a>
            <a href="./practice.html" class="active">Практика</a>
            <a href="./simulators.html">Симуляторы</a>
        </div>

        <h1>Практика</h1>
        <p class="subtitle">Для каждой задачи определите: какое условие каузальности нарушено, почему это проблема, и что нужно сделать.</p>

        <!-- Task 1 -->
        <div class="task-card">
            <div class="task-title">Задача 1. Рандомизация по времени регистрации</div>
            <div class="task-body">
                <p>Команда запускает новый онбординг. Все пользователи, зарегистрированные до 1 марта — контроль (старый онбординг). Все зарегистрированные после 1 марта — тест (новый онбординг). Через месяц сравнивают retention.</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Нарушено:</strong> рандомизация. Это не A/B-тест, а before/after сравнение. Группы различаются по времени регистрации — любой внешний фактор (маркетинговая кампания, сезонность, изменение трафика) confounds результат.</p>
                    <p><strong>Что делать:</strong> рандомизировать по user_id — каждый новый пользователь с равной вероятностью получает старый или новый онбординг, независимо от даты.</p>
                </div>
            </details>
        </div>

        <!-- Task 2 -->
        <div class="task-card">
            <div class="task-title">Задача 2. Маркетплейс: новый алгоритм ранжирования</div>
            <div class="task-body">
                <p>E-commerce маркетплейс тестирует новый алгоритм ранжирования товаров. Рандомизация по покупателям: 50% видят новый алгоритм, 50% — старый. Метрика — GMV per buyer. Через 2 недели тест показывает +8% GMV.</p>
                <p>Но: новый алгоритм показывает товары определённых продавцов чаще. Эти продавцы получают больше продаж, а продавцы, чьи товары ушли ниже — меньше.</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Нарушено:</strong> SUTVA (отсутствие интерференции). Treatment для одних покупателей влияет на товарное предложение для других. Продавцы — общий ресурс: если один покупатель видит товар выше, другой может не увидеть его вовсе.</p>
                    <p><strong>Следствие:</strong> +8% GMV в тесте частично достигнуто за счёт контроля. Реальный ATE при 100% rollout будет значительно ниже — возможно, близок к нулю (zero-sum redistribution).</p>
                    <p><strong>Что делать:</strong> (1) Рандомизация по кластерам (города, регионы). (2) Мониторить метрики продавцов. (3) При rollout сравнить фактический GMV с прогнозом из A/B.</p>
                </div>
            </details>
        </div>

        <!-- Task 3 -->
        <div class="task-card">
            <div class="task-title">Задача 3. CTR нового рекламного формата</div>
            <div class="task-body">
                <p>Команда тестирует новый формат рекламного баннера. Рандомизация по пользователям. Метрика: CTR = клики / показы (глобальное отношение). Результат: CTR вырос на +12%.</p>
                <p>Но: новый формат занимает больше экранного пространства, из-за чего алгоритм показывает <em>меньше</em> баннеров на пользователя (было 8, стало 5).</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Проблема:</strong> treatment-affected exposure. Treatment изменил знаменатель метрики (число показов). CTR вырос не потому, что баннер лучше, а потому, что число показов сократилось.</p>
                    <p><strong>Что делать:</strong> (1) Сравнить абсолютное число кликов per user (не ratio). (2) Сравнить число показов между группами. (3) Если показы различаются — CTR невалиден как primary metric.</p>
                </div>
            </details>
        </div>

        <!-- Task 4 -->
        <div class="task-card">
            <div class="task-title">Задача 4. ARPPU и новый онбординг</div>
            <div class="task-body">
                <p>Эксперимент: новый онбординг увеличивает конверсию в первую покупку. Метрика для оценки эффекта — ARPPU (средняя выручка на покупателя, только среди пользователей, совершивших покупку).</p>
                <p>Результат: ARPPU в тест-группе на 15% <em>ниже</em>, чем в контроле. Вывод аналитика: «новый онбординг ухудшает выручку с покупателя».</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Нарушено:</strong> post-treatment conditioning. ARPPU считается только среди покупателей — это подмножество, определённое post-treatment поведением. Treatment увеличил конверсию &rarr; в тесте больше «новых» покупателей с мелкими чеками &rarr; средний чек падает.</p>
                    <p><strong>Следствие:</strong> ARPPU&thinsp;&darr; не означает, что treatment плох. Total revenue мог вырасти.</p>
                    <p><strong>Что делать:</strong> использовать ITT-метрику (revenue per user, включая нули) как primary. ARPPU — вспомогательная метрика для диагностики.</p>
                </div>
            </details>
        </div>

        <!-- Task 5 -->
        <div class="task-card">
            <div class="task-title">Задача 5. Гео-эксперимент с ценами</div>
            <div class="task-body">
                <p>Компания тестирует повышение цен в приложении доставки еды. Рандомизация по городам: 10 городов — тест, 10 — контроль. Метрика: средний чек.</p>
                <p>Аналитик проводит t-test на уровне пользователей (500K в тесте, 480K в контроле). Получает p&nbsp;=&nbsp;0.0003. Вывод: «повышение цен значимо увеличило средний чек».</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Проблема:</strong> неправильная единица анализа. Рандомизация — по городам (кластерам), а анализ — по пользователям. Эффективный N &asymp; 20 (число городов), а не 980K.</p>
                    <p><strong>Следствие:</strong> t-test с N = 980K драматически занижает p-value. Реальная значимость: p &asymp; 0.15–0.35.</p>
                    <p><strong>Что делать:</strong> cluster-robust SE с кластером = город. Или randomization inference (permutation по 20 городам).</p>
                </div>
            </details>
        </div>

        <!-- Task 6 -->
        <div class="task-card">
            <div class="task-title">Задача 6. Утечка через shared device</div>
            <div class="task-body">
                <p>Эксперимент: новый UI checkout-страницы. Рандомизация по user_id. Семейный аккаунт: несколько членов семьи используют один аккаунт.</p>
                <p>Часть пользователей контрольной группы обсуждают «странную новую страницу оплаты» в семейном чате (screenshot от пользователя из теста).</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Нарушено:</strong> (1) Консистентность — если shared device, то один user_id может означать разных людей. (2) SUTVA — информация о treatment «протекает» в контроль через социальные связи.</p>
                    <p><strong>Следствие:</strong> контроль загрязнён. ATE занижен (diluted).</p>
                    <p><strong>Что делать:</strong> (1) Рандомизация по устройству или cookie. (2) Анализировать кластеры пользователей с общими device_id. (3) При значительном leakage — кластерная рандомизация по household.</p>
                </div>
            </details>
        </div>

        <!-- Task 7 -->
        <div class="task-card">
            <div class="task-title">Задача 7. Социальная сеть: новая фича шеринга</div>
            <div class="task-body">
                <p>Социальная сеть тестирует кнопку «Поделиться» в новом формате. Рандомизация по пользователям. Метрика: число shares per user. Тест-группа: +25% shares.</p>
                <p>Но: когда пользователь тест-группы делится контентом, его друзья (возможно из контроля) видят этот контент и тоже начинают делиться чаще.</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Нарушено:</strong> SUTVA. Социальный граф создаёт интерференцию: treatment одного пользователя влияет на поведение его друзей.</p>
                    <p><strong>Следствие:</strong> +25% — переоценка каузального эффекта. Часть роста вызвана spillover.</p>
                    <p><strong>Что делать:</strong> (1) Graph cluster randomization — рандомизировать связанные компоненты. (2) Замерить степень загрязнения контроля: среди пользователей контроля с >50% друзей в тесте shares выше?</p>
                </div>
            </details>
        </div>

        <!-- Task 8 -->
        <div class="task-card">
            <div class="task-title">Задача 8. Эксперимент с багом в delivery</div>
            <div class="task-body">
                <p>Тест: новый алгоритм распределения заказов по курьерам. Рандомизация по заказам. Метрика: время доставки. Из-за бага 20% заказов в тест-группе обрабатывались старым алгоритмом.</p>
                <p>Результат: среднее время доставки в тесте на 3% лучше, чем в контроле.</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Нарушено:</strong> консистентность. 20% тестовой группы фактически получили контрольный treatment. Наблюдаемый Y для них — Y(0), а не Y(1).</p>
                    <p><strong>Следствие:</strong> attenuation bias — реальный эффект больше 3%. ITT = 3%, LATE = 3% / 0.8 = 3.75%.</p>
                    <p><strong>Что делать:</strong> (1) Исправить баг и перезапустить. (2) Оценить LATE через IV. (3) Раскрыть процент non-compliance в отчёте.</p>
                </div>
            </details>
        </div>

        <!-- Task 9 -->
        <div class="task-card">
            <div class="task-title">Задача 9. Рандомизация по сессиям</div>
            <div class="task-body">
                <p>Эксперимент: новый дизайн страницы каталога. Рандомизация по сессиям: каждая сессия случайно попадает в тест или контроль. Один пользователь в разных сессиях может видеть разные версии.</p>
                <p>Метрика: конверсия (покупка / пользователь).</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Проблема:</strong> единица рандомизации (сессия) &ne; единица анализа (пользователь). Один пользователь может быть и в тесте, и в контроле — нарушение assignment consistency.</p>
                    <p><strong>Следствие:</strong> (1) Attenuation bias. (2) Непоследовательный пользовательский опыт (novelty/confusion effect). (3) Нельзя атрибутировать покупку конкретной версии.</p>
                    <p><strong>Что делать:</strong> рандомизировать по user_id (sticky assignment). Если нужна session-level рандомизация — метрика тоже session-level.</p>
                </div>
            </details>
        </div>

        <!-- Task 10 -->
        <div class="task-card">
            <div class="task-title">Задача 10. AB-тест ценового плана (B2B)</div>
            <div class="task-body">
                <p>B2B SaaS: тестируют новый ценовой план. Рандомизация по компаниям (100 + 100). Метрика: revenue per company. Результат: +22% revenue в тесте.</p>
                <p>Но: 3 крупных enterprise-клиента, попавших в тест, сгенерировали 60% revenue тестовой группы.</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Проблема:</strong> (1) Малая выборка (200 компаний). (2) Экстремальный heavy tail — 3 компании определяют результат. (3) Баланс по размеру случаен и ненадёжен.</p>
                    <p><strong>Следствие:</strong> +22% может быть целиком объяснён тем, что крупные компании случайно попали в тест. Повторная рандомизация может дать &minus;15%.</p>
                    <p><strong>Что делать:</strong> (1) Block randomization по размеру. (2) Winsorization / trimmed mean. (3) Permutation test. (4) Проверить результат без 3 крупнейших компаний.</p>
                </div>
            </details>
        </div>

        <!-- Task 11 -->
        <div class="task-card">
            <div class="task-title">Задача 11. Conditioning on a collider</div>
            <div class="task-body">
                <p>Эксперимент: новая модель рекомендаций. Рандомизация по пользователям. Аналитик анализирует конверсию <em>только среди пользователей, просмотревших &ge;5 товаров</em> (аргумент: «неактивные зашумляют результат»).</p>
                <p>Treatment влияет на то, сколько товаров пользователь просматривает (рекомендации стали релевантнее &rarr; больше просмотров).</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Нарушено:</strong> post-treatment conditioning. «Просмотр &ge;5 товаров» — post-treatment переменная, зависящая от treatment. Фильтрация создаёт разный состав подвыборок.</p>
                    <p><strong>Следствие:</strong> в тесте подвыборка включает «новых активных» пользователей, в контроле — только изначально активных. Сравнение смещено.</p>
                    <p><strong>Что делать:</strong> (1) ITT — анализировать всех. (2) Сегментировать по pre-treatment активности. (3) Просмотры &rarr; secondary metric, не фильтр.</p>
                </div>
            </details>
        </div>

        <!-- Task 12 -->
        <div class="task-card">
            <div class="task-title">Задача 12. Два эксперимента одновременно</div>
            <div class="task-body">
                <p>Две команды одновременно запускают эксперименты на одних и тех же пользователях. Команда A тестирует новый checkout. Команда B — новую цену. Рандомизация независимая.</p>
                <p>Команда A видит +5% CR. Но в подгруппе, где одновременно активен тест B — CR +12%. Без теста B — CR +1%.</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Проблема:</strong> interaction effect. Два treatment взаимодействуют: новый checkout хорошо работает с новой ценой, но плохо — со старой. Средний эффект (+5%) маскирует гетерогенность.</p>
                    <p><strong>Следствие:</strong> если запустить только checkout без новой цены — реальный эффект +1%, а не +5%.</p>
                    <p><strong>Что делать:</strong> (1) Factorial design (2&times;2) — анализировать interaction term. (2) Принимать решение с учётом второго эксперимента. (3) При сильном interaction — запускать последовательно.</p>
                </div>
            </details>
        </div>

        <!-- Task 13 -->
        <div class="task-card">
            <div class="task-title">Задача 13. Survivor bias в retention</div>
            <div class="task-body">
                <p>Эксперимент: новая фича повышает retention. Аналитик измеряет Day 30 retention. Но: в тест-группе 5% пользователей деинсталлировали приложение на Day 3 (из-за багов в новой фиче). Эти пользователи не учитываются в Day 30 retention.</p>
                <p>Результат: Day 30 retention в тесте = 42%, в контроле = 38%. Вывод: «фича улучшает retention».</p>
            </div>
            <details>
                <summary>Ответ</summary>
                <div class="task-answer">
                    <p><strong>Нарушено:</strong> survivor bias (форма post-treatment conditioning). Пользователи, ушедшие из-за бага, исключены из анализа. Оставшиеся — «выжившие», изначально более лояльные.</p>
                    <p><strong>Следствие:</strong> 42% — retention <em>среди выживших</em>. С учётом ушедших: 42% &times; 0.95 = 39.9% — практически как в контроле.</p>
                    <p><strong>Что делать:</strong> (1) ITT: считать retention по всем, включая деинсталлировавших (retention = 0). (2) Мониторить uninstall rate как guardrail. (3) Если uninstall rate выше — red flag.</p>
                </div>
            </details>
        </div>

        <nav class="module-nav">
            <span class="nav-disabled">← Предыдущий модуль</span>
            <a href="../../index.html">К курсу</a>
            <a href="../02_metric_types/index.html">Следующий модуль →</a>
        </nav>
    </div>
</body>
</html>
