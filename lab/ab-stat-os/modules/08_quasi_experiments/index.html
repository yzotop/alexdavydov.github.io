<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Модуль 8. A/B без A/B — Статистика A/B-тестирования</title>
    <link rel="stylesheet" href="../../assets/style.css">
</head>
<body>
    <div class="container">
        <div class="breadcrumbs">
            <a href="../../../../courses/index.html">Все курсы</a> /
            <a href="../../index.html">Статистика A/B-тестирования</a> / Модуль 8
        </div>
        <div class="nav-links">
            <a href="./index.html" class="active">Модуль</a>
            <a href="./practice.html">Практика</a>
            <a href="./simulators.html">Симуляторы</a>
        </div>
        <h1>Модуль 8. A/B без A/B</h1>

        <!-- Section 1 -->
        <section class="section">
            <h2>1. Когда A/B невозможен</h2>

            <p>
                В идеале мы всегда хотели бы запустить аккуратный A/B-тест: случайная рандомизация, чистое разделение трафика, прозрачное сравнение метрик. Но реальность часто ломает этот сценарий. Бывают ситуации, когда провести классический эксперимент <strong>невозможно</strong> — по этическим, организационным или техническим причинам.
            </p>

            <h3>Этические ограничения</h3>
            <p>
                В медицине, финансовых продуктах, страховании и ряде социально значимых сервисов рандомизация может быть просто неприемлемой. Вы не можете случайно «не дать» потенциально полезное лечение части пациентов только ради эксперимента, или случайно навесить повышенную комиссию на часть клиентов банка, чтобы проверить гипотезу про доходность.
            </p>
            <p>
                Регуляторы, этические комитеты и репутационные риски часто делают RCT (randomized controlled trial) невозможным. В таких кейсах приходится опираться на наблюдательные данные и строить <strong>квази-эксперименты</strong>, которые пытаются приблизиться к силе A/B, но без рандомизации.
            </p>

            <h3>Политические и организационные ограничения</h3>
            <p>
                Классический пример: CEO влюбился в новый дизайн и решил выкатить его на всех сразу. Никаких holdout-групп, никаких «давайте подождём две недели». Или регулятор обязывает всех игроков рынка внедрить новую схему тарифов одновременно, без поэтапного теста.
            </p>
            <p>
                В таких ситуациях вы узнаёте о запуске <em>задним числом</em>, уже после rollout-а. Формального контроля нет, но есть данные до и после изменения и, возможно, данные по похожим продуктам или регионам. Именно здесь появляются DiD, synthetic control и прочие методы.
            </p>

            <h3>Технические ограничения</h3>
            <p>
                Иногда фича настолько глубоко вмешивается в инфраструктуру, что разделить трафик просто нельзя. Например:
            </p>
            <ul class="preview-list">
                <li>новая схема кэширования на CDN, которая меняет весь трафик сразу;</li>
                <li>миграция на новую версию базы данных для всего кластера;</li>
                <li>изменение формата события, которое ломает обратную совместимость со старой логикой.</li>
            </ul>
            <p>
                Технически возможен только «big bang»-запуск. При этом бизнес всё равно хочет понять эффект: стало лучше или хуже, насколько сильно.
            </p>

            <h3>Редкие объекты и маленькое N</h3>
            <p>
                Бывают ситуации, где экспериментировать формально можно, но <strong>единиц слишком мало</strong> для полноценной рандомизации. Например:
            </p>
            <ul class="preview-list">
                <li>5 стран, куда вы выходите с новым продуктом;</li>
                <li>3 крупных склада, между которыми идёт перераспределение стока;</li>
                <li>несколько уникальных партнёров, каждый из которых даёт значимую долю выручки.</li>
            </ul>
            <p>
                Делить такое N ещё и на тест/контроль часто бессмысленно: мощность станет нулевой. Тогда приходится работать с наблюдательными данными и «строить контроль» постфактум.
            </p>

            <h3>Сильные сетевые эффекты и интерференция</h3>
            <p>
                В продуктах с network effects (маркетплейсы, соцсети, мессенджеры) user-level рандомизация часто теряет смысл. Если вы включили новую механику только части пользователей, их поведение меняет окружение всех остальных: друзей, подписчиков, продавцов и покупателей.
            </p>
            <p>
                В пределе вы получаете ситуацию, где почти каждый пользователь «смешан» с тестовыми и контрольными соседями, и интерпретация эффекта становится очень сложной. Иногда проще анализировать естественный rollout, чем пытаться соорудить идеальный A/B.
            </p>

            <div class="callout callout-warn">
                <p><strong>Ключевая мысль:</strong> квази-эксперименты — это не замена A/B, а <strong>план Б</strong>. Они дают более слабые каузальные утверждения, требуют дополнительных допущений и проверки устойчивости. Но в мире, где идеальный эксперимент часто невозможен, именно они позволяют сказать что-то осмысленное вместо «ничего не знаем».</p>
            </div>
        </section>

        <!-- Section 2 -->
        <section class="section">
            <h2>2. Difference-in-Differences (DiD)</h2>

            <p>
                Difference-in-Differences (DiD) — один из самых популярных квази-экспериментальных методов. Интуитивно он сравнивает <strong>изменение метрики во времени</strong> в группе, где произошло вмешательство, с изменением той же метрики в похожей группе, где ничего не меняли.
            </p>

            <h3>Интуитивная формула</h3>
            <p>
                Представьте, что у вас есть две группы: treatment (Москва) и control (Санкт-Петербург). Меряем выручку до и после запуска новой цены доставки в Москве:
            </p>
            <div class="formula">
                Эффект ≈ (Y<sub>Москва, post</sub> − Y<sub>Москва, pre</sub>) − (Y<sub>СПб, post</sub> − Y<sub>СПб, pre</sub>)
            </div>
            <p>
                То есть мы берём приращение метрики в treated-группе и «вычитаем» естественное изменение в control-группе. Это естественное изменение служит оценкой того, что произошло бы в treated-группе без вмешательства.
            </p>

            <h3>Ключевое допущение: параллельные тренды</h3>
            <p>
                Сердце DiD — предположение о <strong>параллельных трендах</strong>: если бы не было вмешательства, то обе группы продолжили бы двигаться примерно параллельно во времени. То есть gap между ними оставался бы примерно постоянным (или предсказуемым).
            </p>
            <p>
                Если это допущение нарушено (например, Москва и так росла быстрее СПб до эксперимента), DiD будет путать естественный рост с эффектом фичи.
            </p>

            <h3>Как проверять параллельные тренды</h3>
            <p>
                Практический шаг №1 в любом DiD — нарисовать график метрик по времени <strong>до вмешательства</strong> для treatment и control. Если кривые до момента изменения:
            </p>
            <ul class="preview-list">
                <li>идут примерно параллельно — хорошо, допущение выглядит правдоподобно;</li>
                <li>явно расходятся, одна группа разгоняется быстрее — DiD, скорее всего, будет смещён.</li>
            </ul>
            <p>
                Полезная практика — прогнать «placebo DiD» с ложной датой вмешательства в pre-period. Если уже там вы «находите эффект», значит параллельных трендов нет.
            </p>

            <h3>Пример: новая цена доставки в Москве</h3>
            <p>
                Компания меняет схему цен на доставку только в Москве. В остальных городах тарификация прежняя. У вас есть помесячная выручка по городам за 6 месяцев до и 3 месяца после изменения.
            </p>
            <p>
                Вы строите модель DiD: Москва = treatment, похожие крупные города (СПб, Екатеринбург, Казань) = control. Сравниваете изменение выручки в Москве с изменением в control-группе. Если в control-городах тренд остался прежним, а в Москве произошёл скачок — это сигнал в пользу эффекта новой схемы.
            </p>

            <h3>Риски и источники смещения</h3>
            <ul class="preview-list">
                <li><strong>Специфичные шоки в treated-группе.</strong> Например, в Москву пришла крупная распродажа или локальная PR-кампания, которой не было в СПб.</li>
                <li><strong>Изменение состава пользователей.</strong> В treated-регион мог зайти новый партнёр или ушёл конкурент.</li>
                <li><strong>Неправильный выбор контрольной группы.</strong> Контролем должны быть регионы с максимально похожей динамикой до вмешательства.</li>
            </ul>
            <p>
                Важно также иметь <strong>достаточно длинный pre-period</strong> (часто 4–8 недель минимум), чтобы увидеть стабильный тренд, а не шум из пары точек.
            </p>
        </section>

        <!-- Section 3 -->
        <section class="section">
            <h2>3. Matching / Reweighting</h2>

            <p>
                Второй крупный класс методов — matching и reweighting. Идея проста: мы пытаемся <strong>сконструировать контрольную группу</strong>, максимально похожую на treated-пользователей по наблюдаемым характеристикам.
            </p>

            <h3>Пропенсити-скор и matching</h3>
            <p>
                Пропенсити-скор (propensity score) — это вероятность попасть в treatment при заданных признаках пользователя: активности, устройстве, регионе, источнике трафика и т.д. Мы обучаем модель, которая предсказывает treatment-статус по этим признакам, и затем:
            </p>
            <ul class="preview-list">
                <li>для каждого treated-наблюдения находим похожий по пропенсити контрол;</li>
                <li>или строим группы (strata) по диапазонам пропенсити-скора и сравниваем внутри них.</li>
            </ul>
            <p>
                Интуиция: если два пользователя имеют одинаковый пропенсити-скор, но один получил treatment, а другой нет, то разница в их исходах ближе всего к «локальному эксперименту».
            </p>

            <h3>IPW: взвешивание вместо отбора</h3>
            <p>
                IPW (Inverse Probability Weighting) идёт другим путём: вместо того чтобы отбрасывать часть наблюдений, он <strong>перевзвешивает</strong> контроль так, чтобы его распределение признаков напоминало treatment-группу.
            </p>
            <p>
                Наблюдения, которые «редки» среди контролей, но типичны для treated-пользователей, получают больший вес. В результате мы получаем «синтетическую контрольную группу», похожую на treatment по наблюдаемым факторам.
            </p>

            <h3>Ключевое допущение: selection on observables</h3>
            <p>
                Все matching/reweighting-методы опираются на предположение, что <strong>все важные конфаундеры измерены</strong>. То есть, условно говоря, если два пользователя совпадают по всем учтённым признакам, вероятность treatment для них одинакова, и дальше разница только из-за фичи.
            </p>
            <p>
                Если есть сильные <em>неизмеренные</em> факторы, влияющие и на treatment, и на исход (например, внутренняя мотивация, альфа-пользователь, скрытый сегмент), matching не спасёт — смещение останется.
            </p>

            <h3>Пример: премиум только для «активных» пользователей</h3>
            <p>
                Команда вручную включила премиум только пользователям с &gt; 50 заказов. Теперь вы хотите оценить эффект премиума на удержание. Рандомизации нет, но есть полная история активности.
            </p>
            <p>
                Вы можете подобрать для каждого treated-пользователя похожего не-treated по числу заказов, давности регистрации, устройству, региону, источнику трафика и другим pre-treatment признакам. Если matching удачный, распределения этих признаков в группах совпадут.
            </p>

            <h3>Типичные ошибки</h3>
            <ul class="preview-list">
                <li><strong>Матчинг по post-treatment переменным.</strong> Нельзя матчить по метрикам, на которые treatment уже мог повлиять (например, количеству заказов во время эксперимента).</li>
                <li><strong>Слишком агрессивный отбор.</strong> Если после matching у вас осталось 5% данных, оценка может стать нестабильной и нерепрезентативной.</li>
                <li><strong>Игнорирование невзвешенного баланса.</strong> После matching обязательно проверяйте баланс признаков: сравнивайте средние, строите standardized differences.</li>
            </ul>
        </section>

        <!-- Section 4 -->
        <section class="section">
            <h2>4. Synthetic Control</h2>

            <p>
                Synthetic control — метод для ситуаций, где у вас <strong>очень мало treated-единиц</strong>, иногда всего одна: одна страна, один регион, один склад. Вместо того чтобы делать DiD с «средним контролем», мы строим <strong>взвешенную комбинацию</strong> контролей, которая максимально точно воспроизводит поведение treated-единицы до вмешательства.
            </p>

            <h3>Когда использовать</h3>
            <ul class="preview-list">
                <li>feature запустили только в Германии, а остальные страны ЕС — кандидаты в донорский пул;</li>
                <li>компания открыла новый формат магазина только в одном городе;</li>
                <li>одна платформа перешла на новый тариф, а другие — нет.</li>
            </ul>

            <h3>Идея метода</h3>
            <p>
                Пусть у нас есть Германия (treated) и множество других стран без изменения (доноры). Мы подбираем веса для этих стран так, чтобы их взвешенная комбинация:
            </p>
            <ul class="preview-list">
                <li>максимально точно повторяла траекторию выручки Германии в pre-period;</li>
                <li>была похожа по важным ковариатам (GDP, население, доля онлайн-продаж и т.д.).</li>
            </ul>
            <p>
                Получаем «синтетическую Германию», которая служит контрфактуалом: что было бы, если бы фичу не запускали. После даты вмешательства сравниваем фактическую Германию с synthetic Germany.
            </p>

            <h3>Ключевая проверка: качество pre-period fit</h3>
            <p>
                Главный sanity check — насколько хорошо synthetic control повторяет treated-единицу <strong>до вмешательства</strong>. Если ошибка в pre-period большая, то мы не можем доверять постпериодным различиям: эффект в 5% при pre-period RMSE 15% мало что значит.
            </p>

            <h3>Достоинства и ограничения</h3>
            <ul class="preview-list">
                <li><strong>Плюсы:</strong> метод очень визуальный (график treated vs synthetic), прозрачен по весам доноров, хорошо подходит для коммуникации с бизнесом.</li>
                <li><strong>Минусы:</strong> нужен длинный pre-period (часто 20+ точек во времени) и достаточно широкий донорский пул. При структурных сдвигах (война, ковид, смена регуляции) качество контрфактуала ухудшается.</li>
            </ul>
        </section>

        <!-- Section 5 -->
        <section class="section">
            <h2>5. Regression Discontinuity (RDD)</h2>

            <p>
                Regression Discontinuity Design (RDD) опирается на то, что treatment назначается по формальному порогу непрерывной переменной. Классический пример: пользователи с баллом &gt; 100 получают премиум, с баллом ≤ 100 — нет.
            </p>

            <h3>Интуиция: «почти рандом» около порога</h3>
            <p>
                Пользователи с баллами 99 и 101, скорее всего, <strong>очень похожи</strong> по всем характеристикам. Разница в пару баллов часто случайна. Если assignment строго следует правилу порога и никто им не манипулирует, то около cutoff-а мы получаем почти рандомизированное сравнение.
            </p>

            <h3>Sharp vs fuzzy RDD</h3>
            <ul class="preview-list">
                <li><strong>Sharp RDD:</strong> все пользователи выше порога получают treatment, все ниже — нет.</li>
                <li><strong>Fuzzy RDD:</strong> вероятность treatment резко меняется на пороге, но не становится 0/1 (например, менеджеры иногда выдают премиум вручную).</li>
            </ul>
            <p>
                В обоих случаях мы оцениваем «скачок» метрики в окрестности порога, моделируя зависимость исхода от running variable слева и справа от cutoff-а.
            </p>

            <h3>Риски и проверки</h3>
            <ul class="preview-list">
                <li><strong>Манипуляция порогом.</strong> Если пользователи или система могут «подогнать» балл к порогу (бучинг около 100), то квазиранадомизация ломается.</li>
                <li><strong>Heaping и округления.</strong> Много наблюдений на круглых числах может давать ложное ощущение разрыва.</li>
                <li><strong>Мало наблюдений около cutoff-а.</strong> Истинная мощность RDD определяется числом точек в узком диапазоне вокруг порога.</li>
            </ul>
            <p>
                Практически обязательно:
            </p>
            <ul class="preview-list">
                <li>строить гистограммы running variable около порога;</li>
                <li>проверять баланс ковариат слева и справа от cutoff-а;</li>
                <li>тестировать устойчивость результатов к выбору bandwidth (ширины окна вокруг порога).</li>
            </ul>
        </section>

        <!-- Section 6 -->
        <section class="section">
            <h2>6. Interrupted Time Series (ITS) / Causal Impact</h2>

            <p>
                Interrupted Time Series (ITS) — самый простой квази-эксперимент: у вас есть длинный временной ряд метрики, и в какой-то момент вы вносите изменение (маркетинговая кампания, редизайн, миграция платформы). Вопрос: насколько изменилась траектория ряда относительно того, что можно было бы ожидать без вмешательства.
            </p>

            <h3>Базовая идея</h3>
            <p>
                Мы строим модель, которая пытается описать <strong>поведение ряда до интервенции</strong> (тренд, сезонность, авторегрессии, влияние контрольных рядов), и затем экстраполируем её в постпериод. Разница между фактической траекторией и прогнозом = оценка эффекта.
            </p>

            <h3>CausalImpact и структурные временные ряды</h3>
            <p>
                Google CausalImpact — популярная реализация подхода через Bayesian Structural Time Series. Она:
            </p>
            <ul class="preview-list">
                <li>моделирует тренды и сезонность метрики;</li>
                <li>может включать контрольные ряды (метрики продуктов, где не было вмешательства);</li>
                <li>возвращает распределение контрфактуала и доверительные интервалы для эффекта.</li>
            </ul>

            <h3>Пример: маркетинговая кампания с 1 марта</h3>
            <p>
                Представьте, что 1 марта вы запускаете крупную маркетинговую кампанию и хотите оценить её влияние на DAU. У вас есть помесячный или подневный DAU за год до кампании и несколько контрольных рядов (например, DAU в странах, где кампанию не запускали).
            </p>
            <p>
                CausalImpact строит модель по данным до 1 марта, предсказывает, каким был бы DAU в марте–апреле без кампании, и сравнивает с фактическими значениями. Если фактические DAU стабильно выше верхней границы доверительного интервала контрфактуала, это сигнал эффекта.
            </p>

            <h3>Риски и ограничения</h3>
            <ul class="preview-list">
                <li><strong>Конкурирующие события.</strong> Одновременно с кампанией могли произойти другие изменения: релиз новой фичи, кризис, PR-скандал.</li>
                <li><strong>Сложная сезонность.</strong> Если модель плохо захватывает сезонные паттерны (праздники, выходные), она будет путать сезонный рост с эффектом.</li>
                <li><strong>Сдвиги структуры.</strong> Рынок мог резко измениться из-за внешних факторов, которые модель не видела в прошлом.</li>
            </ul>
            <p>
                ITS особенно полезен, когда <strong>контрольной группы вообще нет</strong> (весь продукт поменяли одновременно), но есть длинная и относительно стабильная история метрики.
            </p>
        </section>

        <!-- Section 7 -->
        <section class="section">
            <h2>7. Как не обмануться: валидация квази-экспериментов</h2>

            <p>
                С квази-экспериментами легко «увидеть то, что хочется». Поэтому критически важно закладывать в анализ проверки, которые помогут отличить реальный эффект от артефакта модели или нарушения допущений.
            </p>

            <h3>Placebo-тесты</h3>
            <p>
                Идея проста: примените ваш метод там, где вы <strong>знаете, что эффекта быть не должно</strong>. Если метод всё равно «находит» значимый uplift, он, скорее всего, ловит структуру шума или трендов, а не реальное влияние фичи.
            </p>
            <ul class="preview-list">
                <li>для DiD — сдвиньте дату «вмешательства» в прошлое и посчитайте эффект по pre-period;</li>
                <li>для synthetic control — возьмите другую страну, где ничего не запускали, и проверьте, не появляются ли ложные эффекты;</li>
                <li>для ITS — примените тот же подход к соседнему окну времени без изменений.</li>
            </ul>

            <h3>Проверка pre-трендов</h3>
            <p>
                Для DiD и synthetic control это обязательный шаг: до вмешательства кривые treated и control (или treated и synthetic) должны вести себя согласованно. Любой систематический разъезд до интервенции — красный флаг.
            </p>

            <h3>Сенситивити-анализ и устойчивость</h3>
            <p>
                Хорошая практика — поиграть с настройками и проверить, насколько стабильна оценка:
            </p>
            <ul class="preview-list">
                <li>для RDD — разный bandwidth, разные полиномы слева и справа от порога;</li>
                <li>для synthetic control — разные наборы доноров, разные веса ковариат;</li>
                <li>для matching — разные наборы признаков и спецификации пропенсити-модели.</li>
            </ul>
            <p>
                Если эффект «исчезает» при небольшом изменении спецификации, доверять ему сложно.
            </p>

            <h3>Selection on unobservables и границы доверия</h3>
            <p>
                Для matching и IPW полезны количественные оценки того, насколько сильным должен быть <strong>неизмеренный конфаундер</strong>, чтобы объяснить наблюдаемый эффект (подходы типа Rosenbaum bounds). Даже если вы не считаете их формально, важно в отчёте явно обсудить, какие скрытые факторы могут ломать каузальную интерпретацию.
            </p>

            <div class="callout">
                <p><strong>Правило:</strong> квази-эксперименты дают <em>суггестивные</em> доказательства, а не железобетонные. В коммуникации с бизнесом всегда оставляйте место для сомнения: «результаты согласованы с гипотезой эффекта, но опираются на допущения X, Y, Z».</p>
            </div>
        </section>

        <!-- Summary table -->
        <section class="section">
            <h2>Итоговая сводка методов</h2>

            <p>
                Сведём основные квази-экспериментальные методы в одну таблицу, чтобы вам было проще выбирать инструмент под задачу.
            </p>

            <table>
                <thead>
                    <tr>
                        <th>Метод</th>
                        <th>Когда применять</th>
                        <th>Ключевое допущение</th>
                        <th>Главный риск</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>DiD</td>
                        <td>Два и более региона/группы, есть данные до и после вмешательства</td>
                        <td>Параллельные тренды: без treatment группы шли бы одинаково</td>
                        <td>Нарушение тренда из-за локальных шоков или плохого контроля</td>
                    </tr>
                    <tr>
                        <td>Matching</td>
                        <td>Выбор в treatment идёт по наблюдаемым характеристикам</td>
                        <td>Selection on observables: все важные конфаундеры измерены</td>
                        <td>Сильные скрытые факторы (unobserved confounders), смещающие оценку</td>
                    </tr>
                    <tr>
                        <td>Synthetic control</td>
                        <td>Одна treated-единица, много донорских контролей и длинный pre-period</td>
                        <td>Существует линейная комбинация доноров, хорошо описывающая pre-period</td>
                        <td>Мало доноров, плохой fit до интервенции, структурные сдвиги в постпериод</td>
                    </tr>
                    <tr>
                        <td>RDD</td>
                        <td>Treatment назначается по порогу непрерывной переменной</td>
                        <td>Отсутствие манипуляции около порога, локальная гладкость связи «метрика–running variable»</td>
                        <td>Манипуляция и бучинг у cutoff-а, мало наблюдений в окрестности порога</td>
                    </tr>
                    <tr>
                        <td>ITS / Causal Impact</td>
                        <td>Нет контрольной группы, но есть длинный временной ряд до интервенции</td>
                        <td>Модель временного ряда адекватно описывает контрфактуальное поведение</td>
                        <td>Одновременные события, плохое моделирование сезонности и внешних шоков</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <!-- Preview sections -->
        <section class="preview-section">
            <h2>Практика</h2>
            <ul class="preview-list">
                <li>14 задач на выбор квази-экспериментального дизайна под бизнес-сценарий</li>
                <li>Диагностика нарушенных допущений: неправильные контролы, манипуляция порогом, плохой synthetic control</li>
                <li>Решения, когда честно признать: «оценить эффект задним числом нельзя»</li>
            </ul>
            <a href="./practice.html" class="nav-card">Открыть практику</a>
        </section>

        <section class="preview-section">
            <h2>Симуляторы</h2>
            <ul class="preview-list">
                <li>Инструменты для исследования распределений и выбора статистических подходов</li>
                <li>Влияние heavy tail, zero inflation и типа метрики на стабильность оценок</li>
                <li>Подготовка к будущим симуляторам по DiD и synthetic control</li>
            </ul>
            <a href="./simulators.html" class="nav-card">Открыть симуляторы</a>
        </section>

        <nav class="module-nav">
            <a href="../07_common_mistakes/index.html">← Предыдущий модуль</a>
            <a href="../../index.html">К курсу</a>
            <span class="nav-disabled">Следующий модуль →</span>
        </nav>
    </div>
</body>
</html>

