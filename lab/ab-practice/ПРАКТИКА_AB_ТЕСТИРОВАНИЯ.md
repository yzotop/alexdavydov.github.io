# Практика A/B-тестирования на графиках

**Смотри график → думай → раскрывай разбор.**

Хороший аналитик видит паттерн до того, как прочитает вывод.

## О курсе

Этот курс — набор сценариев для тренировки интерпретации результатов экспериментов. Каждый сценарий содержит график без подсказок. Ваша задача — самостоятельно проанализировать, что произошло, и только потом раскрыть разбор.

Фокус на мышлении, а не на интерфейсе. Никаких симуляторов и калькуляторов — только графики и ваша способность их читать.

Для аналитиков и продуктовых менеджеров, которые принимают решения на основе экспериментов. После курса вы сможете читать графики экспериментов и видеть механизмы эффектов до того, как прочитаете выводы.

## Как проходить практикум

1. Сначала смотри график и формулируй вывод
2. Запиши 1–2 гипотезы механизма (цена/объём/микс/воронка/давление)
3. Только потом раскрывай разбор
4. Сверяй "неправильный вывод" и "корректный вывод"
5. В конце открывай ссылки на теорию (монетизация/эксперименты)

---

# Сценарии

## 001. CPM вырос, а выручка упала

### Контекст эксперимента

Повысили минимальный floor (порог цены) на 15% в тестовой группе. Цель — улучшить среднюю цену показа и выручку.

Эксперимент длился 14 дней. Трафик стабилен, сезонных эффектов не наблюдалось. Контрольная группа без изменений.

В тестовой группе CPM вырос на 12%, но выручка упала на 3%.

### Подумай

- Что здесь произошло?
- Есть ли эффект от изменения floor?
- Можно ли катить в прод?
- Какие метрики нужно проверить дополнительно?

### Разбор

На графике видно, что CPM действительно вырос после повышения floor. Однако выручка начала падать примерно на 5-й день эксперимента.

**Где глаз обманывается:** Рост CPM создаёт впечатление успеха. Но выручка = CPM × объём показов. Если объём упал сильнее, чем вырос CPM, выручка снизится.

**Альтернативная трактовка:** Floor отрезал низкоценный хвост инвентаря. CPM вырос за счёт того, что низкоценные показы перестали проходить порог. Но общий объём показов упал, потому что часть инвентаря стала непродаваемой.

**Какие метрики вводят в заблуждение:** CPM сам по себе не говорит о здоровье системы. Нужно смотреть на объём показов, fill rate, lost-to-constraints share. Если fill rate упал, а lost-to-constraints вырос — это признак того, что floor слишком высокий.

### Вывод

**❌ Неправильный вывод:**
«CPM вырос на 12%, значит изменение работает. Выручка упала из-за внешних факторов. Можно катить в прод.»

**✅ Корректный вывод:**
Повышение floor привело к росту CPM, но снижению объёма показов. Чистый эффект — падение выручки на 3%. Это указывает на то, что floor слишком высокий и отрезает значительную часть инвентаря. Нельзя катить в прод без снижения floor или расширения доступности инвентаря.

**⚠️ Чего нельзя утверждать:**
Нельзя утверждать, что «высокий floor всегда плох» — это зависит от структуры спроса и доступности инвентаря. Нельзя утверждать, что «эффект проявится позже» — 14 дней достаточно для оценки эффекта floor. Нельзя утверждать, что «контрольная группа нестабильна» без проверки её метрик.

---

## 002. Выручка выросла без роста CPM

### Контекст эксперимента

Изменили механику показа/догрузки (lazy load или изменение видимости блока) в тестовой группе. Цель — рост выручки через увеличение монетизируемых показов.

Эксперимент длился 14 дней. Трафик стабильный, сезонных эффектов не наблюдалось. Контрольная группа без изменений.

В тестовой группе выручка выросла на 4%, CPM остался практически без изменений (~0% или +0.2%). Заметен рост монетизируемых показов и ShowRate.

### Подумай

- Есть ли эффект и когда он начинается?
- За счёт чего могла вырасти выручка при стабильном CPM?
- Какие ранние индикаторы нужно проверить (fill/show, coverage, responses, viewability)?
- Можно ли катить в прод? Какие guardrails важны?

### Разбор

На графике видно, что выручка стабильно растёт с начала эксперимента, в то время как CPM остаётся практически неизменным. Это указывает на то, что рост выручки происходит не за счёт изменения цены, а за счёт увеличения объёма показов.

**Объяснение:** Выручка = CPM × (shows/1000). Если CPM не вырос, рост выручки почти всегда идёт через объём: show rate, fill rate, coverage, монетизируемые показы.

**Типовые механизмы:**
- **Рост show rate:** Блок чаще становится видимым благодаря изменению механики загрузки/видимости. Больше показов = больше выручки при той же цене.
- **Рост fill/coverage:** Больше ответов от рекламных сетей, больше успешных показов. Воронка requests → responses → shows улучшилась.
- **Раскрытие скрытого инвентаря:** Inventory reveal без деградации цены. Инвентарь, который раньше не монетизировался, теперь показывается и приносит выручку.

**Порядок проверки:**
1. Shows / monetizable shows — вырос ли объём показов?
2. Show rate, fill rate — улучшилась ли воронка?
3. Requests → responses → shows — где именно произошло улучшение?
4. Разрезы по плейсментам/девайсам — чтобы исключить mix shift (композиционный сдвиг, который может маскировать деградацию).

### Вывод

**❌ Неправильный вывод:**
«CPM не вырос — значит эффекта нет. Изменение не работает.»

**✅ Корректный вывод:**
Рост выручки на 4% при стабильном CPM указывает на эффект через объём показов, а не через цену. Изменение механики показа привело к росту show rate, fill rate или coverage, что увеличило количество монетизируемых показов. CPM мог не измениться, потому что структура спроса и конкуренция остались прежними — изменилась только доступность инвентаря для монетизации.

**⚠️ Чего нельзя утверждать:**
Нельзя утверждать, что «рынок стал дороже» — CPM не изменился. Нельзя утверждать, что «цена улучшилась» — цена осталась прежней, изменился объём. Нельзя утверждать, что «эффект всегда устойчив» — нужно проверить guardrails (quality, retention, fatigue) и убедиться, что рост объёма не привёл к деградации качества или усталости инвентаря.

---

## 003. Статзначимо, но бизнес-эффекта нет

### Контекст эксперимента

Изменили UI/плейсмент/правило показа в тестовой группе. Primary метрика (CTR) выросла на +0.30% (relative) и стала статистически значимой (p < 0.05).

Guardrail/бизнес-метрика (Revenue/RPM) изменилась на +0.03% (почти ноль, статистически не значимо).

Эксперимент длился 14 дней, трафик стабильный. Контрольная группа без изменений.

Вопрос: можно ли катить в прод на основе статистической значимости primary метрики?

### Подумай

- Почему "stat sig" не означает "можно катить"?
- Как отличить "реальный эффект" от "неважного эффекта"?
- Какая минимальная полезная дельта (MDE / practical significance) здесь нужна?
- Какие guardrails могут запретить выкат даже при росте primary?
- Что нужно проверить по сегментам и по времени (устойчивость эффекта)?

### Разбор

На графике видно, что primary метрика (CTR) действительно выросла в тестовой группе, и разница статистически значима. Однако revenue практически не изменилась, что указывает на отсутствие бизнес-эффекта.

**Разница между статистической и практической значимостью:**
- **Статистическая значимость** (p-value) говорит: "мы уверены, что эффект не ноль". Это техническая уверенность в том, что разница не случайна.
- **Практическая значимость** (practical significance) говорит: "эффект достаточно большой, чтобы влиять на бизнес". Это вопрос величины эффекта, а не уверенности в его существовании.

**Типовые причины расхождения:**
1. **Эффект маленький, но данных много:** При большом объёме данных даже крошечные эффекты становятся статистически значимыми. P-value маленький, но бизнес-импакт близок к нулю.
2. **Primary вырос, но цепочка до денег не сработала:** CTR вырос, но это не конвертировалось в revenue. Возможно, клики не привели к конверсиям, или цена клика упала, или изменился микс инвентаря.
3. **Эффект локальный/сегментный:** Рост в одном сегменте компенсируется просадками в других. Общий эффект на revenue близок к нулю, хотя primary метрика показывает рост.

**Чеклист решения:**
1. **Оценить бизнес-импакт в абсолютных деньгах** (а не процентах): +0.30% CTR при текущем объёме — это сколько дополнительных кликов и сколько денег? Если эффект меньше шума в системе — он неважен.
2. **Сравнить с "шумом"/естественной волатильностью:** Насколько эффект больше естественных колебаний метрики? Если эффект меньше волатильности — он неразличим на практике.
3. **Проверить устойчивость по времени:** Первые дни vs хвост эксперимента. Если эффект ослабевает или исчезает — это признак адаптации, а не устойчивого улучшения.
4. **Проверить сегменты и возможный Simpson:** Разбить по девайсам, гео, плейсментам. Возможно, эффект есть только в одном сегменте, а в других — просадка.
5. **Проверить guardrails:** Pressure, retention, complaints, latency — что уместно для вашей системы. Если guardrails ухудшились, даже маленький рост primary не оправдывает выкат.

**Вывод "катим" возможен только если:**
- Есть measurable бизнес-эффект (revenue/RPM вырос на значимую величину) или есть доказанная механика, которая гарантированно конвертируется в деньги в будущем.
- Риски/guardrails не ухудшились — система остаётся устойчивой.

### Вывод

**❌ Неправильный вывод:**
«Есть stat sig — значит победа, катим. Primary метрика выросла, это хорошо для бизнеса.»

**✅ Корректный вывод:**
Эффект по primary метрике статистически значим, но практически неважен: +0.30% CTR не конвертируется в measurable рост revenue (+0.03% близко к нулю). Бизнес-импакт близок к нулю и меньше естественной волатильности системы. Решение зависит от practical significance (минимальной полезной дельты) и рисков: если нет measurable бизнес-эффекта и guardrails не проверены, выкат не оправдан, даже при статистической значимости.

**⚠️ Чего нельзя утверждать:**
Нельзя утверждать, что "улучшение точно полезно бизнесу" — revenue не выросла. Нельзя утверждать, что "раз p-value маленький, то выручка потом вырастет" — нет доказанной механики конверсии CTR в деньги. Нельзя утверждать, что эффект устойчив без проверки по времени и сегментам — возможно, это адаптация или локальный эффект.

---

## 004. Ничего не произошло — и это результат

### Контекст эксперимента

В тестовой группе изменили механику показа/плейсмент/частоту. Цель — рост выручки (или guardrail: не уронить retention при росте давления).

Тест шёл 14 дней, трафик стабильный. Контрольная группа без изменений.

По основным метрикам (Revenue/RPM/CTR/ShowRate) дельты около 0%. Никаких явных просадок guardrails не видно.

Вопрос: это действительно "нет эффекта" или тест не имел мощности для обнаружения эффекта?

### Подумай

- Это действительно "нет эффекта" или тест не имел мощности (power)?
- Какая минимальная полезная дельта (practical significance) нужна бизнесу?
- Какие красные флаги могут прятаться за "нулём" (лаги, сегменты, каннибализация)?
- Нужно ли повторять тест? Если да — что меняем (дизайн, окно, сегменты, воздействие)?

### Разбор

На графике видно, что обе метрики (Revenue и Primary) остаются практически неизменными в тестовой группе по сравнению с контролем. Разница близка к нулю на протяжении всего эксперимента.

**Три разные ситуации, которые часто путают:**
1. **Эффекта нет (true zero / near-zero):** Изменение действительно не влияет на систему. Механика не работает, гипотеза не подтвердилась. Это валидный результат, если тест имел достаточную мощность для обнаружения эффекта.
2. **Эффект есть, но тест слабый (low power / high variance):** Эффект существует, но тест не смог его обнаружить из-за недостаточной мощности (мало данных, высокая дисперсия, короткое окно). Нужно повторить с большей мощностью или более длинным окном.
3. **Эффект разный по сегментам и взаимно компенсируется (masking / Simpson):** В одних сегментах эффект положительный, в других — отрицательный. В агрегате получается ноль. Нужно проверить сегменты отдельно.

**Минимальный протокол закрытия теста:**
1. **Проверить MDE/power на ключевых метриках:** Хотя бы качественно: могли ли мы увидеть +X%? Если минимальная обнаружимая дельта (MDE) была 5%, а реальный эффект 0.5% — тест не мог его обнаружить. Нужно повторить с большей мощностью.
2. **Проверить устойчивость по времени:** Ранние дни vs хвост эксперимента. Если в начале был эффект, а потом исчез — это адаптация, а не "нет эффекта". Если эффект появился только в конце — это лаг, нужно продлить окно.
3. **Проверить сегменты:** Device/platform/temperature/гео. Возможно, эффект есть в мобильных, но нет в десктопе, или наоборот. Агрегат маскирует эффект.
4. **Проверить механизм (диагностические метрики):** Requests → responses → shows, coverage, fill, show rate. Возможно, механизм сработал частично (например, show rate вырос, но CPM упал), и в итоге revenue не изменилась.
5. **Зафиксировать решение и условия, при которых стоит вернуться к гипотезе:** Если эффекта нет, но гипотеза была разумной — опиши, при каких условиях (больше данных, другой сегмент, другое время) стоит повторить тест.

**Важно:** "Нет эффекта" — полезная информация, если ясно, ЧТО именно не сработало. Это не провал, а результат, который исключает гипотезу или указывает на необходимость изменений в дизайне теста.

### Вывод

**❌ Неправильный вывод:**
«Разницы нет → значит тест бесполезен, забываем. Гипотеза неверна, больше не тестируем.»

**✅ Корректный вывод:**
Нулевой эффект — тоже валидный вывод, если тест имел достаточную мощность для обнаружения эффекта. Нужно описать границы: что мы смогли увидеть (например, эффект больше 2% был бы обнаружен) и что не смогли (эффект меньше 0.5% мог быть пропущен). Решение зависит от проверки: если power достаточен, сегменты проверены, лаги учтены — закрываем тест с выводом "эффекта нет". Если power недостаточен или есть подозрения на компенсацию — повторяем с изменениями (больше данных, другой дизайн, сегментация).

**⚠️ Чего нельзя утверждать:**
Нельзя утверждать, что "гипотеза неверна навсегда" — возможно, эффект есть, но тест не смог его обнаружить. Нельзя утверждать, что "эффект точно 0 при любых условиях" — возможно, эффект есть в других сегментах или проявится позже. Нельзя утверждать, что можно игнорировать лаги/сегменты — нулевой эффект в агрегате может маскировать разнонаправленные эффекты в сегментах или эффекты с лагом.

---

## 005. Сегменты спорят друг с другом (Simpson)

### Контекст эксперимента

В тестовой группе поменяли механику показа/плейсмент. Смотрим метрику RPM (или Revenue) по двум сегментам: Desktop и Touch.

В каждом сегменте тест хуже контроля: Desktop −1.0%, Touch −0.5%.

Но в агрегате (All users) тест лучше контроля: +0.6%.

Эксперимент длился 14 дней, трафик стабильный, но доля сегментов в тесте и контроле различается.

Вопрос: как такое возможно и как принимать решение?

### Подумай

- Как такое возможно, если в каждом сегменте "минус", а в агрегате "плюс"?
- Что в первую очередь проверить: веса сегментов, состав трафика, mix shift?
- Как правильно принимать решение: по all, по сегментам или по целевой аудитории?
- Какие guardrails и риски нужно добавить, чтобы не "вкатить Simpson"?

### Разбор

На графике видно, что агрегатная метрика (All) показывает положительную дельту, в то время как каждый сегмент показывает отрицательную дельту. Это классический пример Simpson's paradox.

**Simpson's paradox простыми словами:** Агрегатная метрика — это взвешенная сумма по сегментам. "Плюс" в агрегате может появиться не из-за улучшения в сегментах, а из-за смены весов (доли Desktop/Touch) между тестом и контролем. Если в тесте больше доля "лучшего" сегмента, агрегат будет лучше, даже если каждый сегмент стал хуже.

**Типовые причины конфликтов сегментов:**
1. **Дисбаланс сплита (разные доли сегментов в тесте/контроле):** Рандомизация не гарантирует одинаковые доли сегментов в тесте и контроле. Если в тесте случайно оказалось больше Desktop (который имеет более высокий RPM), агрегат будет выше, даже если Desktop в тесте хуже, чем Desktop в контроле.
2. **Mix shift по инвентарю/плейсментам внутри сегментов:** Изменение механики показа привело к сдвигу микса инвентаря. Например, в Desktop стало больше высокоценных плейсментов, но их эффективность упала. В Touch стало меньше плейсментов, но их эффективность выросла. Агрегат маскирует эти сдвиги.
3. **Эффект есть только в узком сегменте, а агрегат маскирует:** Эффект есть в небольшом подсегменте (например, мобильные девайсы с определённой версией ОС), но в агрегате Desktop/Touch этот эффект теряется на фоне остальных подсегментов.

**Минимальный протокол:**
1. **Сравнить доли сегментов тест/контроль (weights):** Проверить, одинаковы ли доли Desktop и Touch в тесте и контроле. Если доли различаются — это первый признак проблемы.
2. **Посчитать агрегат как "reweighted" (перевзвесить сегменты на общий baseline):** Выровнять веса сегментов между тестом и контролем. Например, взять общую долю Desktop из обеих групп и пересчитать агрегат. Если после reweight агрегат становится отрицательным — эффект был артефактом весов.
3. **Проверить, не вызван ли эффект сменой состава:** Проверить mix shift внутри сегментов: изменился ли состав инвентаря, плейсментов, устройств. Возможно, эффект вызван не изменением механики, а сменой состава.
4. **Решение принимать по целевой аудитории / guardrails:** Если сегменты показывают отрицательный эффект, а агрегат положительный из-за весов — решение должно быть "не катить", так как продукт ухудшился для целевой аудитории. Если после reweight эффект остаётся положительным — можно рассматривать выкат, но с проверкой guardrails.

**Важно:** "All users" без контроля состава — опасная метрика. Агрегатный эффект может маскировать реальные проблемы в сегментах или создавать ложное впечатление успеха.

### Вывод

**❌ Неправильный вывод:**
«В агрегате плюс — значит катим. Все пользователи выиграли.»

**✅ Корректный вывод:**
Агрегатный плюс (+0.6%) может быть артефактом весов сегментов, а не реальным улучшением. В каждом сегменте эффект отрицательный (Desktop −1.0%, Touch −0.5%), что указывает на ухудшение продукта для целевой аудитории. Нужно выровнять веса сегментов (reweight) и проверить состав трафика. Решение должно приниматься по целевым сегментам и после reweight: если после выравнивания весов эффект остаётся отрицательным — нельзя катить, так как продукт ухудшился для пользователей.

**⚠️ Чего нельзя утверждать:**
Нельзя утверждать, что "продукт улучшился для всех" — в каждом сегменте эффект отрицательный. Нельзя утверждать, что "эффект причинный" без контроля состава — возможно, эффект вызван сменой весов, а не изменением механики. Нельзя утверждать, что "сегменты можно игнорировать" — агрегат без контроля состава маскирует реальные проблемы и может привести к выкату ухудшения.

---

## 006. Окно меняет вывод

### Контекст эксперимента

В тестовой группе изменили механику показа (ленивую догрузку / изменение места блока).

Эксперимент длился 14 дней, трафик стабильный. CPM при этом примерно стабильный, изменения похожи на адаптацию поведения/системы.

В первые 3–4 дня основная метрика (Revenue/RPM) ниже контроля (примерно −1.2%).

Начиная с 7–8 дня метрика выравнивается и к концу теста становится выше (примерно +0.8%).

Вопрос: какой вывод делать и как выбирать окно оценки?

### Подумай

- Какой вывод ты сделаешь на 4-й день? А на 14-й?
- Это "шум", "адаптация", "эффект с лагом" или "смена режима"?
- Какие метрики-диагностики подтвердят механизм лага (show/fill, coverage, latency, частота, усталость)?
- Когда останавливать тест (stop rules), если раннее окно показывает минус?
- Как выбрать окно оценки и что писать в итоговом отчёте?

### Разбор

На графике видно, что дельта меняет знак: первые дни отрицательная, затем пересекает ноль и становится положительной. Это классический паттерн эффекта с лагом.

**Объяснение:** Эффекты могут приходить с лагом по разным причинам: поведение пользователя (адаптация, привыкание), рынок (конкуренция, сезонность), система (кеши, переобучение, прогрев), обучение (инвентарь, рекомендации), раскрытие инвентаря (show rate растёт постепенно).

**Три причины "ранний минус → поздний плюс":**
1. **Пользовательская адаптация:** Поведение меняется постепенно. Пользователи привыкают к новому месту блока, новому формату показа. Первые дни может быть негативная реакция (непривычно, мешает), затем адаптация и улучшение вовлечённости.
2. **Стабилизация системы/торгов:** Переобучение моделей, прогрев кешей, стабилизация торговых алгоритмов. Система "учится" работать с новым форматом, и эффективность растёт со временем.
3. **Раскрытие инвентаря/рост show rate:** Эффект проявляется постепенно. Например, lazy load начинает работать лучше по мере того, как пользователи скроллят дальше, или инвентарь "раскрывается" постепенно, увеличивая show rate.

**Протокол чтения:**
1. **Смотреть не только итоговую дельту, но и форму кривой:** Если кривая меняет знак или направление — это важный сигнал. Нужно понять механизм, а не просто усреднить.
2. **Проверить диагностические метрики:** Shows, show rate, fill/coverage, responses. Если show rate растёт постепенно, а fill rate стабилизируется — это подтверждает механизм лага.
3. **Сравнить 1–3 день vs 10–14 день и понять механизм, а не "среднее":** Не усреднять по всему окну, а разобрать раннее и позднее поведение отдельно. Что изменилось между этими периодами?
4. **Заранее задавать окно и stop rules:** Иначе отчёт станет пост-хок. Если окно выбирается после просмотра данных, это самообман. Нужно заранее определить: какое окно для оценки, какие stop rules для раннего останова.

**Важно:** "Окно меняет вывод" — красный флаг, требующий механистического объяснения. Если вывод зависит от выбора окна, нужно объяснить, почему это происходит, иначе решение будет ненадёжным.

### Вывод

**❌ Неправильный вывод:**
«На 3-й день минус — значит тест провалился, останавливаем. Или наоборот: на 14-й день плюс — значит катим.»

**✅ Корректный вывод:**
Оценка зависит от окна: раннее окно (1–4 день) показывает минус, позднее (10–14 день) — плюс. Если форма кривой меняет знак, нужен разбор механизма и проверка диагностических метрик (show rate, fill, coverage). Решение должно приниматься по заранее заданным правилам (окно и stop rules) и с объяснением лага: почему эффект пришёл позже? Если механизм понятен (адаптация, стабилизация системы, раскрытие инвентаря) и поздний эффект устойчив — можно рассматривать выкат. Если механизм неясен — нужно продлить тест или переработать дизайн.

**⚠️ Чего нельзя утверждать:**
Нельзя утверждать, что "поздний плюс гарантированно устойчив" — возможно, это временная адаптация, которая потом вернётся к минусу. Нельзя утверждать, что "ранний минус был просто шум" — нужно проверить диагностические метрики и понять механизм. Нельзя утверждать, что можно выбирать окно постфактум без риска самообмана — если окно выбирается после просмотра данных, это cherry-picking и ненадёжное решение.

---

## 007. Локальная победа, системный проигрыш

### Контекст эксперимента

В тесте усилили один рекламный формат/плейсмент (подняли приоритет, добавили показы, поменяли место).

Локальная метрика формата улучшилась: Revenue(format A) +6% и/или RPM(format A) +4%.

Но общая выручка (Total Revenue) снизилась: −1.0%.

Второй формат (format B) просел сильнее: Revenue(format B) −4% (или shows/coverage упали).

Эксперимент длился 14 дней, трафик стабильный, CPM по рынку примерно без шоков.

Вопрос: как такое возможно и можно ли "спасти" идею?

### Подумай

- Как так: формат A в плюс, а Total в минус?
- Какие механизмы перетока возможны: вытеснение, частотные ограничения, внимание пользователя, конкуренция за инвентарь?
- Какие диагностические метрики подтвердят каннибализацию: общий объём показов, show/fill, доли форматов, давление, частота?
- Можно ли "спасти" идею: изменить дизайн (cap/приоритет/таргет/порог) вместо полного отказа?

### Разбор

На графике видно, что формат A показывает положительную дельту (+6%), в то время как формат B проседает (−4%), и общая выручка падает (−1%). Это классический паттерн каннибализации.

**Объяснение:** Система ограничена вниманием пользователя и инвентарём. Усиление одного формата часто приводит к:
1. **Вытеснению других показов (substitution):** Формат A "крадёт" показы у формата B. Общий объём показов не растёт, происходит перераспределение.
2. **Повышению давления и снижению эффективности остальных (fatigue):** Увеличение частоты показа формата A повышает общее давление на пользователя, что снижает эффективность формата B и других элементов системы.
3. **Изменению микса в сторону менее эффективной комбинации:** Средний CPM/CTR может выглядеть хорошо, но общая выручка падает из-за сдвига в сторону менее эффективного микса форматов.

**Типовые паттерны:**
1. **"Format A steals impressions from B":** Переток показов. Формат A получил больше показов за счёт формата B. Общий объём показов не изменился или даже упал.
2. **"Pressure increases, efficiency drops":** Убывающая отдача. Увеличение частоты/давления формата A привело к снижению эффективности всей системы. Пользователи устали, CTR упал, CPM снизился.
3. **"Mix shift":** Средний CPM/CTR выглядит хорошо, но total хуже. Смещение микса в сторону формата A (который имеет более высокий локальный RPM) маскирует падение общего объёма или эффективности формата B.

**Протокол проверки:**
1. **Total Revenue и Total shows:** Сначала система, потом детали. Вырос ли общий объём показов или произошёл переток? Если total shows не вырос, а revenue упала — это признак каннибализации.
2. **Revenue/shows по форматам A vs B:** Доли, перетоки. Как изменились доли форматов? Выросла ли доля формата A за счёт формата B? Как изменился объём показов каждого формата?
3. **Pressure/frequency + метрики усталости:** Если есть метрики давления и частоты — проверить, не выросло ли давление и не снизилась ли эффективность из-за усталости.
4. **Show/fill/coverage:** Не деградировала ли воронка? Возможно, увеличение формата A привело к снижению fill rate или coverage других форматов.
5. **Разрезы по сегментам:** Где каннибализация сильнее? Возможно, в одних сегментах эффект положительный, в других — отрицательный, и агрегат маскирует проблему.

**Важно:** Локальные метрики формата нельзя оптимизировать в отрыве от total. Оптимизация одного формата может вредить системе, если она не учитывает ограничения внимания пользователя и инвентаря.

### Вывод

**❌ Неправильный вывод:**
«Format A вырос — значит нужно катить, он же эффективнее. Локальная метрика улучшилась, это хорошо.»

**✅ Корректный вывод:**
Улучшение формата A (+6%) не компенсировало потери формата B (−4%) и системы в целом (−1%). Вероятна каннибализация: формат A "украл" показы у формата B или повысил давление, что снизило эффективность всей системы. Решение — не катить как есть, а переработать дизайн (cap на частоту, изменение приоритета, таргетинг, пороги) и проверить total-first. Если после изменений total revenue не растёт — идею нужно отклонить или существенно доработать.

**⚠️ Чего нельзя утверждать:**
Нельзя утверждать, что "формат A плохой" — он показал рост локально. Нельзя утверждать, что "любое усиление A всегда вредно" — возможно, с правильным дизайном (cap, приоритет) эффект будет положительным. Нельзя утверждать, что можно судить по метрикам одного плейсмента без total — локальная оптимизация может вредить системе, если не учитывает ограничения внимания и инвентаря.

---

# Треки изучения

## Быстрый (30–40 минут)

**Цель:** быстро поймать 4 ключевых ловушки.

1. **001 «CPM вырос, а выручка упала»** — цена не спасает при падении объёма
2. **003 «Статзначимо, но бизнес-эффекта нет»** — stat sig ≠ решение
3. **005 «Сегменты спорят (Simpson)»** — агрегат может врать из-за весов
4. **007 «Локальная победа, системный проигрыш»** — каннибализация и total-first

## Базовый (≈2 часа)

**Цель:** пройти все сценарии 001–007 последовательно.

1. **001 «CPM вырос, а выручка упала»** — учит: цена × объём = выручка, floor может отрезать инвентарь
2. **002 «Выручка выросла без роста CPM»** — учит: эффект через объём (show rate, fill, coverage)
3. **003 «Статзначимо, но бизнес-эффекта нет»** — учит: статистическая ≠ практическая значимость
4. **004 «Ничего не произошло — и это результат»** — учит: как закрывать тесты без эффекта, проверка power/MDE
5. **005 «Сегменты спорят (Simpson)»** — учит: агрегат может маскировать эффекты, нужен reweight
6. **006 «Окно меняет вывод»** — учит: лаги и выбор окна, stop rules
7. **007 «Локальная победа, системный проигрыш»** — учит: каннибализация, total-first подход

## Для лидов (60–90 минут)

**Цель:** тренировать принятие решений и рамку контроля.

1. **006 «Окно меняет вывод»** — stop rules и лаги
2. **005 «Simpson»** — контроль состава и reweight
3. **007 «Local win / system loss»** — total-first и каннибализация

### Чеклист решения (6 вопросов)

1. Какой механизм эффекта?
2. Primary / Diagnostic / Guardrails согласованы?
3. Есть ли риск смещения состава?
4. Что происходит с объёмом (shows/coverage) и воронкой (fill/show)?
5. Устойчив ли эффект по времени/сегментам?
6. Что мониторим после выката?

---

# Чек-лист: чтение экспериментов

Быстрая проверка: формирует ли график эксперимента правильное решение, или блокирует его.

Используйте этот чек-лист перед тем, как принять решение по эксперименту. Если хотя бы один пункт не выполнен — график блокирует решение, даже если все расчёты корректны.

## Механизм эффекта

- [ ] Какой механизм эффекта: цена / объём / микс / воронка / давление?
- [ ] Видно ли механизм на графике без дополнительных вычислений?
- [ ] Согласованы ли Primary / Diagnostic / Guardrails метрики?
- [ ] Что происходит с объёмом (shows/coverage) и воронкой (fill/show)?

## Контроль и состав

- [ ] Есть ли риск смещения состава между контроль и тест?
- [ ] Видно ли эффект в каждом сегменте за 10 секунд?
- [ ] Нужен ли reweight для корректного сравнения?
- [ ] Не маскирует ли агрегат эффекты (Simpson paradox)?

## Время и устойчивость

- [ ] Устойчив ли эффект по времени?
- [ ] Есть ли лаги, которые меняют вывод?
- [ ] Как выбор окна оценки влияет на вывод?
- [ ] Нужны ли stop rules для принятия решения?

## Системные эффекты

- [ ] Есть ли каннибализация между форматами/сегментами?
- [ ] Локальная победа или системный проигрыш?
- [ ] Что происходит с Total, а не только с локальными метриками?
- [ ] Что мониторим после выката?

## Статистика и решение

- [ ] Статистическая значимость ≠ практическая значимость?
- [ ] Есть ли бизнес-эффект, даже если stat sig?
- [ ] Можно ли принять решение на основе графика за 10 секунд?
- [ ] Понятно ли, какое действие требуется (запускать, откатывать, итерация)?

---

# Рекомендуемая связка с теорией

После прохождения сценариев рекомендуется изучить теоретические основы для более глубокого понимания механизмов.

- Метрики для чтения эксперимента: primary vs diagnostic vs guardrails
- Время и лаги: когда эффект проявляется
- CPM: цена и её декомпозиция
- Show rate и fill rate: воронка монетизации

