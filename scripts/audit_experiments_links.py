#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–∏—Ç—ã—Ö —Å—Å—ã–ª–æ–∫ –≤ /lab/experiments/ –∏ /docs/experiments_md/

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python3 scripts/audit_experiments_links.py

–°–∫—Ä–∏–ø—Ç –ø—Ä–æ–≤–µ—Ä—è–µ—Ç:
- HTML —Ñ–∞–π–ª—ã –≤ lab/experiments/ (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏)
- MD —Ñ–∞–π–ª—ã –≤ docs/experiments_md/ (—Å—Å—ã–ª–∫–∏ –Ω–∞ .html —Ñ–∞–π–ª—ã)

–ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –≤–Ω–µ—à–Ω–∏–µ URL, —è–∫–æ—Ä—è, –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –ø—É—Ç–∏ –∏ query-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã.
"""

import os
import re
import sys
from pathlib import Path
from urllib.parse import urlparse, unquote
from html.parser import HTMLParser
from collections import defaultdict

# –ö–æ—Ä–µ–Ω—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (–≥–¥–µ –∑–∞–ø—É—â–µ–Ω —Å–∫—Ä–∏–ø—Ç)
REPO_ROOT = Path(__file__).parent.parent
EXPERIMENTS_DIR = REPO_ROOT / "lab" / "experiments"
EXPERIMENTS_MD_DIR = REPO_ROOT / "docs" / "experiments_md"

# –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã —Å—Å—ã–ª–æ–∫
IGNORE_PREFIXES = ["http://", "https://", "mailto:", "#", "/"]


class LinkExtractor(HTMLParser):
    """–ü–∞—Ä—Å–µ—Ä HTML –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤—Å–µ—Ö href –∞—Ç—Ä–∏–±—É—Ç–æ–≤"""
    
    def __init__(self, source_file):
        super().__init__()
        self.source_file = source_file
        self.links = []  # [(line_number, href), ...]
        self.current_line = 1
        
    def handle_starttag(self, tag, attrs):
        if tag == "a":
            for attr_name, attr_value in attrs:
                if attr_name == "href" and attr_value:
                    self.links.append((self.current_line, attr_value))
    
    def handle_data(self, data):
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É \n
        self.current_line += data.count('\n')


def normalize_path(href, source_file):
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É.
    –£–±–∏—Ä–∞–µ—Ç —è–∫–æ—Ä—è (#), query (?), –¥–µ–∫–æ–¥–∏—Ä—É–µ—Ç URL.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∞–±—Å–æ–ª—é—Ç–Ω—ã–π Path –∏–ª–∏ None –µ—Å–ª–∏ –ø—É—Ç—å –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π.
    """
    # –£–±–∏—Ä–∞–µ–º —è–∫–æ—Ä—å –∏ query
    href_clean = href.split('#')[0].split('?')[0]
    
    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º URL
    href_clean = unquote(href_clean)
    
    if not href_clean:
        return None
    
    # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –∏ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –ø—É—Ç–∏
    for prefix in IGNORE_PREFIXES:
        if href_clean.startswith(prefix):
            return None
    
    # –ü–æ–ª—É—á–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    source_dir = source_file.parent
    
    # –†–∞–∑—Ä–µ—à–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å
    try:
        resolved = (source_dir / href_clean).resolve()
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø—É—Ç—å –≤–Ω—É—Ç—Ä–∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
        try:
            resolved.relative_to(REPO_ROOT)
            return resolved
        except ValueError:
            return None
    except (OSError, ValueError):
        return None


def check_file_exists(file_path):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞"""
    return file_path.exists() and file_path.is_file()


def extract_links_from_markdown(md_file):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –∏–∑ Markdown —Ñ–∞–π–ª–∞"""
    links = []
    try:
        with open(md_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ [text](url)
        pattern = r'\[([^\]]+)\]\(([^)]+)\)'
        for match in re.finditer(pattern, content):
            href = match.group(2)
            line_num = content[:match.start()].count('\n') + 1
            links.append((line_num, href))
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {md_file}: {e}")
    
    return links


def audit_links():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Å—ã–ª–æ–∫"""
    
    all_broken_links = []
    html_files = []
    md_files = []
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º HTML —Ñ–∞–π–ª—ã
    if EXPERIMENTS_DIR.exists():
        html_files = list(EXPERIMENTS_DIR.glob("*.html"))
        if html_files:
            print(f"üìÅ –ü—Ä–æ–≤–µ—Ä—è—é {len(html_files)} HTML —Ñ–∞–π–ª–æ–≤ –≤ {EXPERIMENTS_DIR}")
            broken_html = audit_html_files(html_files)
            all_broken_links.extend(broken_html)
            print()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º MD —Ñ–∞–π–ª—ã
    if EXPERIMENTS_MD_DIR.exists():
        md_files = list(EXPERIMENTS_MD_DIR.glob("*.md"))
        if md_files:
            print(f"üìÅ –ü—Ä–æ–≤–µ—Ä—è—é {len(md_files)} MD —Ñ–∞–π–ª–æ–≤ –≤ {EXPERIMENTS_MD_DIR}")
            broken_md = audit_markdown_files(md_files)
            all_broken_links.extend(broken_md)
            print()
    
    # –í—ã–≤–æ–¥–∏–º –æ—Ç—á—ë—Ç
    print_report(html_files, md_files, all_broken_links)
    
    if not all_broken_links:
        print("‚úÖ –í—Å–µ —Å—Å—ã–ª–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã!")
        return 0
    
    return 1


def audit_html_files(html_files):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Å—ã–ª–∫–∏ –≤ HTML —Ñ–∞–π–ª–∞—Ö"""
    all_links = []
    broken_links = []
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª
    for html_file in sorted(html_files):
        try:
            with open(html_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            parser = LinkExtractor(html_file)
            parser.feed(content)
            
            for line_num, href in parser.links:
                all_links.append((html_file, line_num, href))
                
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—É—Ç—å
                resolved_path = normalize_path(href, html_file)
                
                if resolved_path is None:
                    # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –≤–Ω–µ—à–Ω–∏–µ/–∞–±—Å–æ–ª—é—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ
                if not check_file_exists(resolved_path):
                    broken_links.append({
                        'source_file': html_file.relative_to(REPO_ROOT),
                        'line_number': line_num,
                        'href_original': href,
                        'resolved_path': resolved_path.relative_to(REPO_ROOT)
                    })
        
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {html_file}: {e}")
    
    return broken_links


def audit_markdown_files(md_files):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Å—ã–ª–∫–∏ –≤ Markdown —Ñ–∞–π–ª–∞—Ö"""
    all_links = []
    broken_links = []
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª
    for md_file in sorted(md_files):
        links = extract_links_from_markdown(md_file)
        
        for line_num, href in links:
            all_links.append((md_file, line_num, href))
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ —Å—Å—ã–ª–∫–∏ –Ω–∞ .html —Ñ–∞–π–ª—ã
            if not href.endswith('.html'):
                continue
            
            # –î–ª—è MD —Ñ–∞–π–ª–æ–≤ —Å—Å—ã–ª–∫–∏ –¥–æ–ª–∂–Ω—ã —Ä–∞–∑—Ä–µ—à–∞—Ç—å—Å—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ lab/experiments/
            # –∞ –Ω–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ docs/experiments_md/
            if href.startswith('./'):
                # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è —Å—Å—ã–ª–∫–∞ - —Ä–∞–∑—Ä–µ—à–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ lab/experiments/
                target_file = EXPERIMENTS_DIR / href[2:]
            elif href.startswith('../'):
                # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è —Å—Å—ã–ª–∫–∞ –≤–≤–µ—Ä—Ö - —Ä–∞–∑—Ä–µ—à–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ lab/experiments/
                target_file = (EXPERIMENTS_DIR / href).resolve()
            elif href.startswith('/lab/experiments/'):
                # –ê–±—Å–æ–ª—é—Ç–Ω–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ –∫—É—Ä—Å
                target_file = REPO_ROOT / href[1:]  # –£–±–∏—Ä–∞–µ–º –≤–µ–¥—É—â–∏–π /
            else:
                # –î—Ä—É–≥–∏–µ —Å—Å—ã–ª–∫–∏ (–≤–Ω–µ—à–Ω–∏–µ, –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ) - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ
            if not check_file_exists(target_file):
                broken_links.append({
                    'source_file': md_file.relative_to(REPO_ROOT),
                    'line_number': line_num,
                    'href_original': href,
                    'resolved_path': target_file.relative_to(REPO_ROOT) if target_file.exists() or str(target_file).startswith(str(REPO_ROOT)) else str(target_file)
                })
    
    return broken_links


def print_report(html_files, md_files, broken_links):
    """–í—ã–≤–æ–¥–∏—Ç –æ—Ç—á—ë—Ç –æ –ø—Ä–æ–≤–µ—Ä–∫–µ"""
    print("=" * 80)
    print("üìä –û–¢–ß–Å–¢ –û –ü–†–û–í–ï–†–ö–ï –°–°–´–õ–û–ö")
    print("=" * 80)
    
    total_files = 0
    total_links = 0
    
    if html_files:
        total_files += len(html_files)
    if md_files:
        total_files += len(md_files)
    
    print(f"‚úÖ –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {total_files}")
    print(f"‚ùå –ë–∏—Ç—ã—Ö —Å—Å—ã–ª–æ–∫: {len(broken_links)}")
    print()
    
    if broken_links:
        print("‚ùå –ë–ò–¢–´–ï –°–°–´–õ–ö–ò:")
        print("-" * 80)
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ñ–∞–π–ª—É-–∏—Å—Ç–æ—á–Ω–∏–∫—É
        by_source = defaultdict(list)
        for link in broken_links:
            by_source[str(link['source_file'])].append(link)
        
        for source_file in sorted(by_source.keys()):
            print(f"\nüìÑ {source_file}:")
            for link in sorted(by_source[source_file], key=lambda x: x['line_number']):
                print(f"   –°—Ç—Ä–æ–∫–∞ {link['line_number']:4d}: {link['href_original']}")
                print(f"              ‚Üí {link['resolved_path']} (–Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)")
        
        print()
        print("=" * 80)
        print("üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–°–ü–†–ê–í–õ–ï–ù–ò–Æ:")
        print("=" * 80)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        patterns_files = ['pressure-patterns.html', 'time-patterns.html', 'decision-patterns.html']
        found_patterns = False
        
        for link in broken_links:
            href_file = Path(link['href_original']).name
            if href_file in patterns_files:
                found_patterns = True
                break
        
        if found_patterns:
            print("\n‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ *-patterns.html —Ñ–∞–π–ª—ã:")
            print("   –≠—Ç–∏ —Ñ–∞–π–ª—ã –±—ã–ª–∏ —É–¥–∞–ª–µ–Ω—ã –∏–ª–∏ –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–ª–∏.")
            print("\n   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
            print("   1. –ï—Å–ª–∏ —Å—Å—ã–ª–∫–∞ –≤ index.html ‚Äî —É–¥–∞–ª–∏—Ç—å –∫–∞—Ä—Ç–æ—á–∫—É/–±–ª–æ–∫ —Å —ç—Ç–æ–π —Å—Å—ã–ª–∫–æ–π")
            print("   2. –ï—Å–ª–∏ —Å—Å—ã–ª–∫–∞ –≤ –¥—Ä—É–≥–æ–º —Ñ–∞–π–ª–µ ‚Äî –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —É—Ä–æ–∫:")
            print("      - pressure-patterns.html ‚Üí pressure-design.html")
            print("      - time-patterns.html ‚Üí time-and-lags.html")
            print("      - decision-patterns.html ‚Üí metric-conflicts.html")
            print()
        
        print("   –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        print("   - –£–¥–∞–ª–∏—Ç—å –±–∏—Ç—ã–µ —Å—Å—ã–ª–∫–∏ –∏–ª–∏ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã")
        print("   - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π (../, ./)")
        print("   - –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –≤—Å–µ —Ñ–∞–π–ª—ã –∫—É—Ä—Å–∞ —Å–æ–∑–¥–∞–Ω—ã")
        print()
        
        return 1
    else:
        print("‚úÖ –í—Å–µ —Å—Å—ã–ª–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã!")
        print()
        return 0


if __name__ == "__main__":
    try:
        exit_code = audit_links()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
